{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "# AMF Stress Test â€” workflow JOUR PAR JOUR (base = exposures)\n",
    "# ================================================================\n",
    "# Ce script :\n",
    "# 1) charge le mapping (POS + scenario_paths),\n",
    "# 2) charge 'exposures' (greeks) + nettoie/contrÃ´le qualitÃ© (QC),\n",
    "# 3) restreint les scÃ©narios au pÃ©rimÃ¨tre d'exposures (par Identifier),\n",
    "# 4) applique les chocs pour un jour donnÃ© (day_step_apply),\n",
    "# 5) renvoie un 'exposures_next' prÃªt Ã  Ãªtre MODIFIÃ‰ avant le jour suivant.\n",
    "# ------------------------------------------------\n",
    "# CLÃ‰ DE MERGE / AGRÃ‰GATION = Identifier  (ISIN conservÃ© pour info)\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collateral_management import process_pv_after_day_1, roll_balance_for_next_day\n",
    "\n",
    "# =======================\n",
    "# CONFIG â€” Ã  adapter\n",
    "# =======================\n",
    "EXCEL_MAPPING_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\PÃ©rimÃ¨tre et positions\\Matrices correspondance_AB.xlsx\"\n",
    "SHEET_POS = \"Test_Aya\"   # entÃªtes ligne 2 -> header=1\n",
    "SHEET_SCEN =  \"scenario_paths\"\n",
    "POS_HEADER_ROW = 1               # 0-based\n",
    "\n",
    "EXPOSURES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\PÃ©rimÃ¨tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\"\n",
    "TRIOPTIMA_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\PÃ©rimÃ¨tre et positions\\search_groupama-am_2025-03-31.xlsx\"\n",
    "COLLATERAL_BALANCES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\PÃ©rimÃ¨tre et positions\\Collat_Cash_MTM_LU_20250401.csv\" \n",
    "COLLATERAL_BALANCE_DAY_COL = \"Balance J\"\n",
    "COLLATERAL_BALANCE_PREV_COL = \"Balance J-1\"\n",
    "COLLATERAL_THRESHOLD_COL = \"Seuil dÃ©clenchement\"\n",
    "FUTURE_ASSET_CLASSES = {\n",
    "       'Bond Future',\n",
    "        'Equity Index Future'\n",
    "        'FX Future'}\n",
    "\n",
    "# Jours possibles (doivent exister dans scenario_paths)\n",
    "DAYS = [\"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "# RÃ¨gles de mÃ©thode\n",
    "INCLUDE_OTHER_INFLATION_SWAP = True\n",
    "\n",
    "# =======================\n",
    "# Utils colonnes / texte\n",
    "# =======================\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie des colonnes Excel (espaces, 'Unnamed', points).\"\"\"\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c is None or (isinstance(c, str) and c.lower().startswith(\"unnamed\")):\n",
    "            new_cols.append(None); continue\n",
    "        s = str(c).strip().replace(\"\\u00A0\", \" \")\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace(\". \", \" \").replace(\".\", \" \")\n",
    "        new_cols.append(s)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def _norm_str(x):\n",
    "    \"\"\"Normalise lÃ©gÃ¨rement une chaÃ®ne (pour Market / Variable).\"\"\"\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().replace(\"\\u00A0\", \" \")\n",
    "    s = \" \".join(s.split()).replace(\". \", \" \").replace(\".\", \" \")\n",
    "    return s\n",
    "\n",
    "# =======================\n",
    "# Chargement mapping (POS + scen)\n",
    "# =======================\n",
    "POS_BASE_COLS = [\n",
    "    \"Identifier\",\"ISIN\",\"Counterparty\",\"Description\",\"Currency\",\"AssetType\",\"Sector1\",\"Seniority\",\n",
    "    \"CompositeBroadRating\",\"MaturityDate\",\"Maturity\",\"Maturity Band\",\"EffectiveMaturityDate\",\n",
    "    \"LiquidityScore\",\"Country\",\"{Class_Rating}\"\n",
    "]\n",
    "POS_MV_PAIRS = [(f\"Market {i}\", f\"Variable {i}\") for i in range(1, 7)]\n",
    "SCEN_BASE_COLS = [\"Market\",\"Variable\",\"Comment\",\"Type\",\"Unit\",\"T0\",\n",
    "                  \"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "def load_mapping(path_excel: str|Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lit la feuille POS et scenario_paths.\"\"\"\n",
    "    xls = pd.ExcelFile(path_excel)\n",
    "    pos  = pd.read_excel(xls, SHEET_POS, header=POS_HEADER_ROW)\n",
    "    scen = pd.read_excel(xls, SHEET_SCEN)\n",
    "    pos  = _clean_cols(pos)\n",
    "    scen = _clean_cols(scen)\n",
    "    return pos, scen\n",
    "\n",
    "def melt_pos(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforme POS en format long : 1 ligne par (Market k, Variable k) non vide.\"\"\"\n",
    "    pos = pos.copy()\n",
    "    for mk, vk in POS_MV_PAIRS:\n",
    "        if mk in pos.columns: pos[mk] = pos[mk].apply(_norm_str)\n",
    "        if vk in pos.columns: pos[vk] = pos[vk].apply(_norm_str)\n",
    "\n",
    "    base_cols = [c for c in POS_BASE_COLS if c in pos.columns]\n",
    "    mv_pairs_present = [(mk,vk) for mk,vk in POS_MV_PAIRS if mk in pos.columns and vk in pos.columns]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in pos.iterrows():\n",
    "        base = {c: row.get(c, np.nan) for c in base_cols}\n",
    "        for mk, vk in mv_pairs_present:\n",
    "            market, variable = row[mk], row[vk]\n",
    "            if pd.notna(market) and str(market) != \"\":\n",
    "                rows.append({**base, \"Market\": market, \"Variable\": (variable if pd.notna(variable) else np.nan)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"Identifier\" not in out.columns: out[\"Identifier\"] = np.arange(len(out))\n",
    "    return out\n",
    "\n",
    "def prepare_scenarios(scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie la table scenario_paths & conserve les colonnes utiles.\"\"\"\n",
    "    scen = scen.copy()\n",
    "    scen = scen[[c for c in SCEN_BASE_COLS if c in scen.columns]]\n",
    "    scen[\"Market\"] = scen[\"Market\"].apply(_norm_str)\n",
    "    if \"Variable\" in scen.columns: scen[\"Variable\"] = scen[\"Variable\"].apply(_norm_str)\n",
    "    if \"Type\" in scen.columns:     scen[\"Type\"]     = scen[\"Type\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    if \"Unit\" in scen.columns:     scen[\"Unit\"]     = scen[\"Unit\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    return scen\n",
    "\n",
    "def merge_pos_scen(pos_long: pd.DataFrame, scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Relie POS (long) aux scÃ©narios : jointure (Market,Variable) ; si Variable vide -> jointure sur Market seul.\"\"\"\n",
    "    left_mv = pos_long.dropna(subset=[\"Market\",\"Variable\"]) if \"Variable\" in pos_long.columns else pos_long.copy()\n",
    "    mv_merge = left_mv.merge(scen, on=[\"Market\",\"Variable\"], how=\"left\")\n",
    "    if \"Variable\" in pos_long.columns: left_m = pos_long[pos_long[\"Variable\"].isna()].copy()\n",
    "    else:                               left_m = pd.DataFrame(columns=pos_long.columns)\n",
    "    if not left_m.empty:\n",
    "        m_merge = left_m.merge(scen.drop(columns=[\"Variable\"], errors=\"ignore\"), on=\"Market\", how=\"left\")\n",
    "        return pd.concat([mv_merge, m_merge], ignore_index=True)\n",
    "    return mv_merge\n",
    "\n",
    "def available_days(scen: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Retourne la liste des colonnes 'Day n' disponibles.\"\"\"\n",
    "    return [c for c in scen.columns if isinstance(c, str) and c.lower().startswith(\"day\")]\n",
    "\n",
    "# =======================\n",
    "# Limiter les scÃ©narios au pÃ©rimÃ¨tre d'exposures (clÃ© = Identifier)\n",
    "# =======================\n",
    "def restrict_scenarios_to_exposures(merged_mapping: pd.DataFrame,\n",
    "                                    exposures: pd.DataFrame,\n",
    "                                    key_col: str = \"Identifier\") -> pd.DataFrame:\n",
    "    \"\"\"Garde uniquement les lignes de mapping dont la clÃ© existe dans exposures.\"\"\"\n",
    "    if key_col not in merged_mapping.columns or key_col not in exposures.columns:\n",
    "        return merged_mapping\n",
    "    keys = exposures[key_col].astype(str).unique()\n",
    "    return merged_mapping[merged_mapping[key_col].astype(str).isin(keys)].copy()\n",
    "\n",
    "# =======================\n",
    "# Standardisation des chocs\n",
    "# =======================\n",
    "def standardize_shock(value, unit: str|None) -> float|None:\n",
    "    \"\"\"Choc standardisÃ© en dÃ©cimal: 50 bps -> 0.005 ; -10% -> -0.10 ; 2 p.p -> 0.02.\"\"\"\n",
    "    if pd.isna(value): return None\n",
    "    try: val = float(value)\n",
    "    except: return None\n",
    "    if unit is None: return val\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return val / 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\"]:\n",
    "        return val / 100.0\n",
    "    return val\n",
    "\n",
    "def _to_bps(shock_std: float, unit: str|None) -> float:\n",
    "    \"\"\"Convertit un choc standardisÃ© en bps numÃ©riques si besoin (pour PV01 / CS01 / Infl01).\"\"\"\n",
    "    if shock_std is None or pd.isna(shock_std) or unit is None: return np.nan\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return shock_std * 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\",\"pc\"]:\n",
    "        return shock_std * 10_000.0\n",
    "    return np.nan\n",
    "\n",
    "def build_daily_shocks(merged: pd.DataFrame, day_col: str) -> pd.DataFrame:\n",
    "    \"\"\"PrÃ©pare les chocs d'un jour (inclut Identifier & ISIN si prÃ©sents).\"\"\"\n",
    "    if day_col not in merged.columns:\n",
    "        raise ValueError(f\"Jour '{day_col}' introuvable. Jours dispo: {available_days(merged)}\")\n",
    "    out = merged.copy()\n",
    "    out[\"shock_raw\"] = out[day_col]\n",
    "    out[\"shock_std\"] = [standardize_shock(v, u) for v, u in zip(out[\"shock_raw\"], out.get(\"Unit\", pd.Series([None]*len(out))))]\n",
    "\n",
    "    keep = [\"Identifier\",\"ISIN\",\"Market\",\"Variable\",\"Type\",\"Unit\",\"T0\", day_col, \"shock_std\",\"Comment\"]\n",
    "    keep = [c for c in keep if c in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# =======================\n",
    "# Chargement exposures + QC\n",
    "# =======================\n",
    "def load_exposures(path_excel: str|Path, return_qc: bool = True):\n",
    "    \"\"\"Charge le fichier d'expositions, nettoie et met des valeurs par dÃ©faut pour Ã©viter les NaN.\"\"\"\n",
    "    df = pd.read_csv(path_excel,sep=';',decimal='.')\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    qc = {\"path\": str(path_excel)}\n",
    "    qc[\"rows_raw\"] = int(df.shape[0])\n",
    "\n",
    "    # Convertir en numÃ©rique les colonnes clÃ©s si elles existent\n",
    "    num_cols = [\n",
    "        \"TV\",\"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\n",
    "        \"RateDelta1bp\",\"RateVega\",\"SpreadDelta1bp\",\"CreditVega\",\n",
    "        \"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\",\n",
    "        \"Nominal\",\"TVPercent\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Enlever les lignes agrÃ©gÃ©es 'TOTAL' si jamais elles existent\n",
    "    removed_total = 0\n",
    "    if \"AssetID\" in df.columns:\n",
    "        mask_total = df[\"AssetID\"].astype(str).str.upper().eq(\"TOTAL\")\n",
    "        removed_total = int(mask_total.sum())\n",
    "        df = df[~mask_total].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    qc[\"rows_removed_total\"] = removed_total\n",
    "    qc[\"rows_after_filter\"] = int(df.shape[0])\n",
    "\n",
    "    mask_0 = df['Nominal'] == 0\n",
    "    removed_n_0 = int(mask_0.sum())\n",
    "    df = df[~mask_0].copy()\n",
    "\n",
    "    qc['row_removes_notional_null'] = removed_n_0\n",
    "\n",
    "    # Sensi NaN -> 0 (Ã©vite les effets NaN)\n",
    "    sensi_zero = [\n",
    "        \"RateDelta1bp\",\"SpreadDelta1bp\",\"InflationDelta1bp\",\n",
    "        \"EquityDelta\",\"FXDelta\",\"RateVega\",\"CreditVega\",\"EquityVega\",\"FXVega\",\"EquityGamma\"\n",
    "    ]\n",
    "    qc[\"filled_zero\"] = {}\n",
    "    for c in sensi_zero:\n",
    "        if c in df.columns:\n",
    "            n = int(df[c].isna().sum())\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "            qc[\"filled_zero\"][c] = n\n",
    "\n",
    "    # Duration NaN -> 0\n",
    "    if \"Duration\" in df.columns:\n",
    "        qc[\"duration_filled_zero\"] = int(df[\"Duration\"].isna().sum())\n",
    "        df[\"Duration\"] = df[\"Duration\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"duration_missing_col\"] = True\n",
    "        df[\"Duration\"] = 0.0\n",
    "\n",
    "    # DollarRateConvexity1pc : NaN -> 0\n",
    "    if \"DollarRateConvexity1pc\" in df.columns:\n",
    "        qc[\"dollarconvexity_filled_zero\"] = int(df[\"DollarRateConvexity1pc\"].isna().sum())\n",
    "        df[\"DollarRateConvexity1pc\"] = df[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"dollarconvexity_missing_col\"] = True\n",
    "        df[\"DollarRateConvexity1pc\"] = 0.0\n",
    "\n",
    "    return (df, qc) if return_qc else df\n",
    "\n",
    "def print_qc_report(qc: dict):\n",
    "    \"\"\"Affiche un petit rapport de nettoyage des expositions.\"\"\"\n",
    "    print(f\"ðŸ“„ Fichier : {qc.get('path','')}\")\n",
    "    print(f\"ðŸ“¦ Lignes brutes : {qc['rows_raw']}\")\n",
    "    print(f\"ðŸ§¹ Lignes 'TOTAL' supprimÃ©es : {qc['rows_removed_total']}\")\n",
    "    print(f\"âœ… Lignes aprÃ¨s filtre : {qc['rows_after_filter']}\")\n",
    "    print(\"ðŸ”§ NaN â†’ 0 (sensis) :\")\n",
    "    for k, v in qc[\"filled_zero\"].items():\n",
    "        print(f\"  - {k:<22}: {v}\")\n",
    "    if \"duration_filled_zero\" in qc:\n",
    "        print(f\"\\nâ±  Duration NaN â†’ 0 : {qc['duration_filled_zero']}\")\n",
    "    if qc.get(\"duration_missing_col\"):\n",
    "        print(\"âš ï¸  Colonne 'Duration' manquante â†’ crÃ©Ã©e Ã  0\")\n",
    "    if qc.get(\"dollarconvexity_missing_col\"):\n",
    "        print(\"âš ï¸  Colonne 'DollarRateConvexity1pc' manquante â†’ crÃ©Ã©e\")\n",
    "    if \"dollarconvexity_filled_zero\" in qc:\n",
    "        print(f\"ðŸ“ DollarRateConvexity1pc NaN â†’ 0 : {qc['dollarconvexity_filled_zero']}\")\n",
    "        \n",
    "def load_counterparty_mapping(path_excel: str|Path, id_col='FREE_TEXT_1', cp_col='CP_ORIG') -> pd.DataFrame:\n",
    "    \"\"\"Lit le fichier Trioptima et renvoie un mapping Identifierâ†’Counterparty.\"\"\"\n",
    "    df = pd.read_excel(path_excel)\n",
    "    df = _clean_cols(df)\n",
    "    if id_col not in df.columns or cp_col not in df.columns:\n",
    "        raise KeyError(f\"Colonnes '{id_col}' ou '{cp_col}' manquantes dans {path_excel}\")\n",
    "    return (df[[id_col, cp_col]]\n",
    "            .dropna(subset=[id_col])\n",
    "            .rename(columns={id_col: 'Identifier', cp_col: 'Counterparty'})\n",
    "            .drop_duplicates('Identifier'))\n",
    "\n",
    "# =======================\n",
    "# Ã‰tape \"un jour\" â€” base = exposures, clÃ© = Identifier\n",
    "# =======================\n",
    "def day_step_apply(\n",
    "    exposures: pd.DataFrame,\n",
    "    merged_mapping: pd.DataFrame,\n",
    "    day_col: str = \"Day 1\",\n",
    "    include_other_inflation_swap: bool = True,\n",
    "    update_duration: bool = True,   # Duration_next = Duration_t + DollarRateConvexity1pc/100 Ã— shock_rates_dec\n",
    "    update_tv: bool = True,         # TV_day = TV + TotalEffect\n",
    "    return_pivot: bool = True,\n",
    "    key_col: str = \"Identifier\",    # clÃ© de jointure\n",
    "    port_col: str = \"Portfolio\",    # colonne de portefeuille\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique les chocs d'un jour et renvoie:\n",
    "      - detailed       : lignes (Portfolio Ã— Identifier Ã— Market Ã— Variable) avec Effect/Method (+ ISIN si dispo)\n",
    "      - per_id         : agrÃ©gat par Portefeuille/Identifier (ISIN=first), TV_day, (Duration_next si update_duration)\n",
    "      - exposures_next : copie de exposures avec TV/Duration mises Ã  jour (selon flags)\n",
    "      - pivot          : (optionnel) large des Effects ('Market :: Variable'), index = (Portefeuille, Identifier)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Chocs du jour (standardisÃ©s en dÃ©cimal)\n",
    "    shocks_day = build_daily_shocks(merged_mapping, day_col=day_col)  # contient Identifier, ISIN (POS), Market, Variable, shock_std...\n",
    "    shocks_day = shocks_day.drop_duplicates(subset=[key_col, \"Market\", \"Variable\"])\n",
    "    # 2) Merge en LEFT depuis exposures (la base de calcul) par Identifier et ajoute le Portefeuille\n",
    "    cols_needed = [\n",
    "        key_col, port_col, \"AssetClass\",\"Country\",\"AssetID\",\"Nominal\",\"TVPercent\",\"TV\",\n",
    "        \"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\"RateDelta1bp\",\"RateVega\",\n",
    "        \"SpreadDelta1bp\",\"CreditVega\",\"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\", \"Counterparty\"\n",
    "    ]\n",
    "    cols_needed = [c for c in cols_needed if c in exposures.columns]\n",
    "    base = exposures[cols_needed].copy()\n",
    "    if key_col not in base.columns or key_col not in shocks_day.columns:\n",
    "        raise KeyError(f\"ClÃ© '{key_col}' absente de exposures ou du mapping de scÃ©narios.\")\n",
    "\n",
    "    df = base.merge(shocks_day, on=key_col, how=\"left\")  # garde les instruments sans mapping (effet=0)\n",
    "\n",
    "    # 3) Calcul des effets par ligne (Portfolio Ã— Identifier Ã— Market Ã— Variable)\n",
    "    effects, methods = [], []\n",
    "    for _, r in df.iterrows():\n",
    "        market_raw = r.get(\"Market\")\n",
    "        if isinstance(market_raw, str):\n",
    "            market = market_raw.lower()\n",
    "        elif pd.notna(market_raw):\n",
    "            market = str(market_raw).lower()\n",
    "        else:\n",
    "            market = \"\"\n",
    "        variable = (r.get(\"Variable\") or \"\")\n",
    "        unit     = r.get(\"Unit\")\n",
    "        shock_std = r.get(\"shock_std\", np.nan)\n",
    "\n",
    "        shock_bps = _to_bps(shock_std, unit)  # pour PV01/CS01/Infl01 (en bps numÃ©riques)\n",
    "        shock_dec = shock_std                 # dÃ©cimal (ex: +50 bps -> +0.005)\n",
    "\n",
    "        effect = np.nan\n",
    "        method = None\n",
    "\n",
    "        if market == \"equity\":\n",
    "            eq_delta = r.get(\"EquityDelta\", np.nan)\n",
    "            if pd.notna(eq_delta) and pd.notna(shock_dec):\n",
    "                effect = eq_delta * shock_dec\n",
    "                method = \"EquityDelta Ã— shock_dec\"\n",
    "\n",
    "        elif market == \"interest rates\":\n",
    "            if pd.notna(r.get(\"RateDelta1bp\")) and pd.notna(shock_bps):\n",
    "                effect = r[\"RateDelta1bp\"] * shock_bps\n",
    "                method = \"RateDelta1bp Ã— shock_bps\"\n",
    "\n",
    "        elif \"spread\" in market:  # couvre Gov Spreads / Corp Spreads (peu importe la casse)\n",
    "            sp01 = r.get(\"SpreadDelta1bp\", np.nan)\n",
    "            if pd.notna(sp01) and pd.notna(shock_bps):\n",
    "                effect = sp01 * shock_bps\n",
    "                method = \"SpreadDelta1bp Ã— shock_bps\"\n",
    "\n",
    "        elif market == \"fx\":\n",
    "            fx_delta = r.get(\"FXDelta\", np.nan)\n",
    "            if pd.notna(fx_delta) and pd.notna(shock_dec):\n",
    "                effect = fx_delta * shock_dec\n",
    "                method = \"FXDelta Ã— shock_dec\"\n",
    "\n",
    "        elif market == \"other\":\n",
    "            # Cas spÃ©cifique demandÃ©: Other + Variable = Inflation Swap\n",
    "            if isinstance(variable, str) and \"inflation swap\" in variable.lower():\n",
    "                infl01 = r.get(\"InflationDelta1bp\", np.nan)\n",
    "                if pd.notna(infl01) and pd.notna(shock_bps):\n",
    "                    effect = infl01 * shock_bps\n",
    "                    method = \"InflationDelta1bp Ã— shock_bps (Other/Inflation Swap)\"\n",
    "\n",
    "        effects.append(effect)\n",
    "        methods.append(method)\n",
    "\n",
    "    df[\"Effect\"] = effects\n",
    "    df[\"Method\"] = methods\n",
    "    df[\"Effect\"] = df[\"Effect\"].fillna(0.0)  # pas de mapping -> effet 0\n",
    "\n",
    "    # 4) AgrÃ©gat par Portefeuille/Identifier (on conserve ISIN = first pour info)\n",
    "    agg = {\n",
    "        \"TV\": (\"TV\",\"first\"),\n",
    "        \"Duration_t\": (\"Duration\",\"first\"),\n",
    "        \"DollarRateConvexity1pc\": (\"DollarRateConvexity1pc\",\"first\"),\n",
    "        \"TotalEffect\": (\"Effect\",\"sum\"),\n",
    "        \"Counterparty\":(\"Counterparty\",\"first\")\n",
    "    }\n",
    "    if \"ISIN\" in df.columns:\n",
    "        agg[\"ISIN\"] = (\"ISIN\",\"first\")\n",
    "\n",
    "    if \"AssetClass\" in df.columns:\n",
    "        agg[\"AssetClass\"] = (\"AssetClass\",\"first\")\n",
    "\n",
    "    \n",
    "\n",
    "    per_id = df.groupby([port_col, key_col], as_index=False).agg(**agg) \n",
    "    per_id[\"TV_day\"] = per_id[\"TV\"] + per_id[\"TotalEffect\"]\n",
    "\n",
    "    # 5) (option) mise Ã  jour de la Duration : Duration_next = Duration_t + DollarRateConvexity1pc/100 Ã— (âˆ‘choc_rates_dec)\n",
    "    if update_duration:\n",
    "        rates_mask = df[\"Market\"].fillna(\"\").str.lower().eq(\"interest rates\")\n",
    "        rates_choc_dec = (df.loc[rates_mask]\n",
    "                            .groupby([port_col, key_col], as_index=False)[\"shock_std\"]\n",
    "                            .sum()\n",
    "                            .rename(columns={\"shock_std\":\"shock_rates_dec\"}))\n",
    "        per_id = per_id.merge(rates_choc_dec, on=[port_col, key_col], how=\"left\")\n",
    "        per_id[\"shock_rates_dec\"] = per_id[\"shock_rates_dec\"].fillna(0.0)\n",
    "        per_id[\"DollarRateConvexity1pc\"] = per_id[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "        per_id[\"Duration_t\"]      = per_id[\"Duration_t\"].fillna(0.0)\n",
    "        per_id[\"Duration_next\"]   = per_id[\"Duration_t\"] + (per_id['Duration_t']**2 -per_id[\"DollarRateConvexity1pc\"] /per_id['TV']* 10000) * per_id[\"shock_rates_dec\"]\n",
    "\n",
    "        futures_mask = per_id[\"AssetClass\"].isin(FUTURE_ASSET_CLASSES)\n",
    "        per_id.loc[futures_mask, \"Duration_next\"] = per_id.loc[futures_mask, \"Duration_t\"]\n",
    "    # 6) Construire exposures_next (mise Ã  jour TV/Duration par (Portefeuille, Identifier))\n",
    "\n",
    "\n",
    "    exposures_next = exposures.copy()\n",
    "    merge_cols = [port_col, key_col]\n",
    "    update_cols = merge_cols + [\"TV_day\"]\n",
    "    if update_duration:\n",
    "        update_cols.append(\"Duration_next\")\n",
    "        \n",
    "    exposures_next = exposures_next.merge(\n",
    "        per_id[update_cols],\n",
    "        on=merge_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    exposures_next[\"TV\"] = exposures_next[\"TV_day\"].fillna(exposures_next[\"TV\"])\n",
    "    if update_duration and \"Duration_next\" in exposures_next.columns:\n",
    "        futures_mask = exposures_next[\"AssetClass\"].isin(FUTURE_ASSET_CLASSES)\n",
    "        duration_updates = exposures_next[\"Duration_next\"].fillna(exposures_next[\"Duration\"])\n",
    "        exposures_next.loc[~futures_mask, \"Duration\"] = duration_updates.loc[~futures_mask]\n",
    "\n",
    "    exposures_next = exposures_next.drop(columns=[c for c in [\"TV_day\", \"Duration_next\"] if c in exposures_next.columns])\n",
    "\n",
    "\n",
    "    return df, per_id, exposures_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Fichier : C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\PÃ©rimÃ¨tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\n",
      "ðŸ“¦ Lignes brutes : 222\n",
      "ðŸ§¹ Lignes 'TOTAL' supprimÃ©es : 2\n",
      "âœ… Lignes aprÃ¨s filtre : 220\n",
      "ðŸ”§ NaN â†’ 0 (sensis) :\n",
      "  - RateDelta1bp          : 1\n",
      "  - SpreadDelta1bp        : 40\n",
      "  - InflationDelta1bp     : 191\n",
      "  - FXDelta               : 181\n",
      "\n",
      "â±  Duration NaN â†’ 0 : 10\n",
      "ðŸ“ DollarRateConvexity1pc NaN â†’ 0 : 24\n",
      "TV avant remise Ã  zÃ©ro par AssetClass (futures):\n",
      "            AssetClass  TV_before_reset\n",
      "0          Bond Future       3686943.19\n",
      "1  Equity Index Future             0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Charger mapping & scÃ©narios\n",
    "pos_raw, scen_raw = load_mapping(EXCEL_MAPPING_PATH)\n",
    "scen_raw=scen_raw.iloc[:29,:]\n",
    "\n",
    "\n",
    "# 2) Charger exposures (greeks) + QC\n",
    "exposures, qc = load_exposures(EXPOSURES_PATH, return_qc=True)\n",
    "print_qc_report(qc)\n",
    "\n",
    "# Ajouter les contreparties depuis Trioptima\n",
    "cp_map = load_counterparty_mapping(TRIOPTIMA_PATH)\n",
    "exposures = exposures.merge(cp_map, on='Identifier', how='left')\n",
    "\n",
    "# Limiter POS au pÃ©rimÃ¨tre d'exposures avant melt\n",
    "pos_subset = exposures[['Identifier','Counterparty']].merge(pos_raw, on='Identifier', how='left')\n",
    "# 3) PrÃ©parer mapping restreint & scÃ©narios \n",
    "pos_long = melt_pos(pos_subset)\n",
    "scen     = prepare_scenarios(scen_raw)\n",
    "merged   = merge_pos_scen(pos_long, scen)          # mapping POS â†” scÃ©narios\n",
    "\n",
    "\n",
    "# 4) DAY 1\n",
    "d1_det, d1_id, exp_d2 = day_step_apply(\n",
    "    exposures=exposures,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 1\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# AgrÃ©gation des TV et remise Ã  zÃ©ro pour certains futures\n",
    "updated_exp, futures_tv, balances, decisions, alerts = process_pv_after_day_1(exp_d2)\n",
    "print(\"TV avant remise Ã  zÃ©ro par AssetClass (futures):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        print(futures_tv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions.to_csv(\"decisions.csv\",sep=\";\",decimal=\".\",index=False)\n",
    "d1_id.to_csv(\"d1_id.csv\",sep=\";\",decimal=\".\",index=False)\n",
    "d1_det.to_csv(\"d1_det.csv\",sep=\";\",decimal=\".\",index=False)\n",
    "exp_d2.to_csv(\"exp_d2.csv\",sep=\";\",decimal=\".\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    'AssetType',\n",
    "    'Sector1',\n",
    "    'Seniority',\n",
    "    'CompositeBroadRating',\n",
    "    'MaturityDate',\n",
    "    'Maturity',\n",
    "    'Maturity Band',\n",
    "    'EffectiveMaturityDate',\n",
    "    'LiquidityScore',\n",
    "    'Country',\n",
    "    '{Class_Rating}',\n",
    "]\n",
    "def merge_day1_positions(exposures: pd.DataFrame, day1_per_id: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine current exposures with Dayâ€‘1 results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposures : pd.DataFrame\n",
    "        DataFrame containing the current positions. Must include an\n",
    "        ``Identifier`` column so that classification fields can be merged.\n",
    "    day1_per_id : pd.DataFrame\n",
    "        ``per_id`` DataFrame returned by :func:`day_step_apply`. If the\n",
    "        identifier column is named ``Identifier`` it will be normalised to\n",
    "        ``d1_id``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dayâ€‘1 positions enriched with classification columns and a\n",
    "        ``TV_change`` column equal to ``TV - TV_day`` when both are available.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'Identifier' in day1_per_id.columns and 'd1_id' not in day1_per_id.columns:\n",
    "        day1_per_id = day1_per_id.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    class_cols = [c for c in COLUMNS_TO_KEEP if c in exposures.columns]\n",
    "    class_df = exposures[['Identifier'] + class_cols].drop_duplicates('Identifier')\n",
    "    class_df = class_df.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    merged = day1_per_id.merge(class_df, on='d1_id', how='left')\n",
    "\n",
    "    if 'TV' in merged.columns and 'TV_day' in merged.columns:\n",
    "        merged['TV_change'] = merged['TV'] - merged['TV_day']\n",
    "\n",
    "    return merged\n",
    "\n",
    "def aggregate_positions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate positions by the available classification columns.\n",
    "\n",
    "    All numeric trade value columns found in the dataframe are summed. The\n",
    "    function looks for ``TV`` (current value), ``TV_day`` (day-one value) and the\n",
    "    computed ``TV_change``. When none of these columns are present a simple\n",
    "    count per group is returned instead.\n",
    "    \"\"\"\n",
    "\n",
    "    group_cols = [col for col in COLUMNS_TO_KEEP if col in df.columns]\n",
    "\n",
    "    agg_spec = {}\n",
    "    for col in ('TV', 'TV_day', 'TV_change'):\n",
    "        if col in df.columns:\n",
    "            agg_spec[col] = 'sum'\n",
    "\n",
    "    if agg_spec:\n",
    "        agg = df.groupby(group_cols, dropna=False).agg(agg_spec).reset_index()\n",
    "    else:\n",
    "        agg = df.groupby(group_cols, dropna=False).size().reset_index(name='count')\n",
    "\n",
    "    return agg\n",
    "\n",
    "def process_positions_df(exposures: pd.DataFrame, d1_id: pd.DataFrame, aggregate: bool = False) -> pd.DataFrame:\n",
    "    #Merge Day 1 results from ``day_step_apply`` with exposures.\n",
    "    df = merge_day1_positions(exposures, d1_id)\n",
    "    if aggregate:\n",
    "        df = aggregate_positions(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country groupings for government bond aggregation\n",
    "EUROZONE_LOW_RISK = {\n",
    "    'Austria', 'Belgium', 'Finland', 'Germany', 'Ireland', 'Latvia', 'Luxembourg', 'Netherlands', 'Slovenia'\n",
    "}\n",
    "EUROZONE_MEDIUM_RISK = {\n",
    "    'Croatia', 'Cyprus', 'France', 'Lithuania', 'Malta', 'Portugal', 'Slovakia'\n",
    "}\n",
    "EUROZONE_HIGH_RISK = {\n",
    "    'Greece', 'Italy', 'Spain'\n",
    "}\n",
    "\n",
    "Emerging_MARKETS = {\n",
    "    'Argentina', 'Brazil', 'Chile', 'China', 'Colombia', 'India', 'Indonesia', 'Mexico', 'Peru', 'South Africa', 'Turkey'\n",
    "}\n",
    "\n",
    "Advanced_economics_MARKETS = {'United States', 'Canada' ,\n",
    "                               'United Kingdom', 'Germany', 'France', 'Italy', 'Spain', 'Netherlands', 'Sweden', 'Switzerland', 'Norway', 'Austria', 'Belgium', 'Denmark', 'Finland', 'Ireland', 'Portugal', 'Greece', 'Czech Republic', 'Slovakia', 'Slovenia', 'Estonia', 'Latvia', 'Lithuania', 'Luxembourg', 'Croatia', 'Cyprus', 'Malta','Andorra', 'San Marino',\n",
    "                              'Japan', 'South Korea', 'Australia', 'New Zealand', 'Singapore', 'Hong Kong SAR', 'Taiwan', 'Macao SAR',\n",
    "                            'Israel', 'Iceland', 'Puerto Rico' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
