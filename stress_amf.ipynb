{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "# AMF Stress Test ‚Äî workflow JOUR PAR JOUR (base = exposures)\n",
    "# ================================================================\n",
    "# Ce script :\n",
    "# 1) charge le mapping (POS + scenario_paths),\n",
    "# 2) charge 'exposures' (greeks) + nettoie/contr√¥le qualit√© (QC),\n",
    "# 3) restreint les sc√©narios au p√©rim√®tre d'exposures (par Identifier),\n",
    "# 4) applique les chocs pour un jour donn√© (day_step_apply),\n",
    "# 5) renvoie un 'exposures_next' pr√™t √† √™tre MODIFI√â avant le jour suivant.\n",
    "# ------------------------------------------------\n",
    "# CL√â DE MERGE / AGR√âGATION = Identifier  (ISIN conserv√© pour info)\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =======================\n",
    "# CONFIG ‚Äî √† adapter\n",
    "# =======================\n",
    "EXCEL_MAPPING_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\Matrices correspondance.xlsx\"\n",
    "SHEET_POS = \"Test_Aya\"   # ent√™tes ligne 2 -> header=1\n",
    "SHEET_SCEN = \"scenario_paths\"\n",
    "POS_HEADER_ROW = 1               # 0-based\n",
    "\n",
    "EXPOSURES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\"\n",
    "TRIOPTIMA_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\search_groupama-am_2025-03-31.xlsx\"\n",
    "\n",
    "# Jours possibles (doivent exister dans scenario_paths)\n",
    "DAYS = [\"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "# R√®gles de m√©thode\n",
    "INCLUDE_OTHER_INFLATION_SWAP = True\n",
    "\n",
    "# =======================\n",
    "# Utils colonnes / texte\n",
    "# =======================\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie des colonnes Excel (espaces, 'Unnamed', points).\"\"\"\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c is None or (isinstance(c, str) and c.lower().startswith(\"unnamed\")):\n",
    "            new_cols.append(None); continue\n",
    "        s = str(c).strip().replace(\"\\u00A0\", \" \")\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace(\". \", \" \").replace(\".\", \" \")\n",
    "        new_cols.append(s)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def _norm_str(x):\n",
    "    \"\"\"Normalise l√©g√®rement une cha√Æne (pour Market / Variable).\"\"\"\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().replace(\"\\u00A0\", \" \")\n",
    "    s = \" \".join(s.split()).replace(\". \", \" \").replace(\".\", \" \")\n",
    "    return s\n",
    "\n",
    "# =======================\n",
    "# Chargement mapping (POS + scen)\n",
    "# =======================\n",
    "POS_BASE_COLS = [\n",
    "    \"Identifier\",\"ISIN\",\"Counterparty\",\"Description\",\"Currency\",\"AssetType\",\"Sector1\",\"Seniority\",\n",
    "    \"CompositeBroadRating\",\"MaturityDate\",\"Maturity\",\"Maturity Band\",\"EffectiveMaturityDate\",\n",
    "    \"LiquidityScore\",\"Country\",\"{Class_Rating}\"\n",
    "]\n",
    "POS_MV_PAIRS = [(f\"Market {i}\", f\"Variable {i}\") for i in range(1, 7)]\n",
    "SCEN_BASE_COLS = [\"Market\",\"Variable\",\"Comment\",\"Type\",\"Unit\",\"T0\",\n",
    "                  \"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "def load_mapping(path_excel: str|Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lit la feuille POS et scenario_paths.\"\"\"\n",
    "    xls = pd.ExcelFile(path_excel)\n",
    "    pos  = pd.read_excel(xls, SHEET_POS, header=POS_HEADER_ROW)\n",
    "    scen = pd.read_excel(xls, SHEET_SCEN)\n",
    "    pos  = _clean_cols(pos)\n",
    "    scen = _clean_cols(scen)\n",
    "    return pos, scen\n",
    "\n",
    "def melt_pos(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforme POS en format long : 1 ligne par (Market k, Variable k) non vide.\"\"\"\n",
    "    pos = pos.copy()\n",
    "    for mk, vk in POS_MV_PAIRS:\n",
    "        if mk in pos.columns: pos[mk] = pos[mk].apply(_norm_str)\n",
    "        if vk in pos.columns: pos[vk] = pos[vk].apply(_norm_str)\n",
    "\n",
    "    base_cols = [c for c in POS_BASE_COLS if c in pos.columns]\n",
    "    mv_pairs_present = [(mk,vk) for mk,vk in POS_MV_PAIRS if mk in pos.columns and vk in pos.columns]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in pos.iterrows():\n",
    "        base = {c: row.get(c, np.nan) for c in base_cols}\n",
    "        for mk, vk in mv_pairs_present:\n",
    "            market, variable = row[mk], row[vk]\n",
    "            if pd.notna(market) and str(market) != \"\":\n",
    "                rows.append({**base, \"Market\": market, \"Variable\": (variable if pd.notna(variable) else np.nan)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"Identifier\" not in out.columns: out[\"Identifier\"] = np.arange(len(out))\n",
    "    return out\n",
    "\n",
    "def prepare_scenarios(scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie la table scenario_paths & conserve les colonnes utiles.\"\"\"\n",
    "    scen = scen.copy()\n",
    "    scen = scen[[c for c in SCEN_BASE_COLS if c in scen.columns]]\n",
    "    scen[\"Market\"] = scen[\"Market\"].apply(_norm_str)\n",
    "    if \"Variable\" in scen.columns: scen[\"Variable\"] = scen[\"Variable\"].apply(_norm_str)\n",
    "    if \"Type\" in scen.columns:     scen[\"Type\"]     = scen[\"Type\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    if \"Unit\" in scen.columns:     scen[\"Unit\"]     = scen[\"Unit\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    return scen\n",
    "\n",
    "def merge_pos_scen(pos_long: pd.DataFrame, scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Relie POS (long) aux sc√©narios : jointure (Market,Variable) ; si Variable vide -> jointure sur Market seul.\"\"\"\n",
    "    left_mv = pos_long.dropna(subset=[\"Market\",\"Variable\"]) if \"Variable\" in pos_long.columns else pos_long.copy()\n",
    "    mv_merge = left_mv.merge(scen, on=[\"Market\",\"Variable\"], how=\"left\")\n",
    "    if \"Variable\" in pos_long.columns: left_m = pos_long[pos_long[\"Variable\"].isna()].copy()\n",
    "    else:                               left_m = pd.DataFrame(columns=pos_long.columns)\n",
    "    if not left_m.empty:\n",
    "        m_merge = left_m.merge(scen.drop(columns=[\"Variable\"], errors=\"ignore\"), on=\"Market\", how=\"left\")\n",
    "        return pd.concat([mv_merge, m_merge], ignore_index=True)\n",
    "    return mv_merge\n",
    "\n",
    "def available_days(scen: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Retourne la liste des colonnes 'Day n' disponibles.\"\"\"\n",
    "    return [c for c in scen.columns if isinstance(c, str) and c.lower().startswith(\"day\")]\n",
    "\n",
    "# =======================\n",
    "# Limiter les sc√©narios au p√©rim√®tre d'exposures (cl√© = Identifier)\n",
    "# =======================\n",
    "def restrict_scenarios_to_exposures(merged_mapping: pd.DataFrame,\n",
    "                                    exposures: pd.DataFrame,\n",
    "                                    key_col: str = \"Identifier\") -> pd.DataFrame:\n",
    "    \"\"\"Garde uniquement les lignes de mapping dont la cl√© existe dans exposures.\"\"\"\n",
    "    if key_col not in merged_mapping.columns or key_col not in exposures.columns:\n",
    "        return merged_mapping\n",
    "    keys = exposures[key_col].astype(str).unique()\n",
    "    return merged_mapping[merged_mapping[key_col].astype(str).isin(keys)].copy()\n",
    "\n",
    "# =======================\n",
    "# Standardisation des chocs\n",
    "# =======================\n",
    "def standardize_shock(value, unit: str|None) -> float|None:\n",
    "    \"\"\"Choc standardis√© en d√©cimal: 50 bps -> 0.005 ; -10% -> -0.10 ; 2 p.p -> 0.02.\"\"\"\n",
    "    if pd.isna(value): return None\n",
    "    try: val = float(value)\n",
    "    except: return None\n",
    "    if unit is None: return val\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return val / 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\"]:\n",
    "        return val / 100.0\n",
    "    return val\n",
    "\n",
    "def _to_bps(shock_std: float, unit: str|None) -> float:\n",
    "    \"\"\"Convertit un choc standardis√© en bps num√©riques si besoin (pour PV01 / CS01 / Infl01).\"\"\"\n",
    "    if shock_std is None or pd.isna(shock_std) or unit is None: return np.nan\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return shock_std * 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\",\"pc\"]:\n",
    "        return shock_std * 10_000.0\n",
    "    return np.nan\n",
    "\n",
    "def build_daily_shocks(merged: pd.DataFrame, day_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Pr√©pare les chocs d'un jour (inclut Identifier & ISIN si pr√©sents).\"\"\"\n",
    "    if day_col not in merged.columns:\n",
    "        raise ValueError(f\"Jour '{day_col}' introuvable. Jours dispo: {available_days(merged)}\")\n",
    "    out = merged.copy()\n",
    "    out[\"shock_raw\"] = out[day_col]\n",
    "    out[\"shock_std\"] = [standardize_shock(v, u) for v, u in zip(out[\"shock_raw\"], out.get(\"Unit\", pd.Series([None]*len(out))))]\n",
    "    keep = [\"Identifier\",\"ISIN\",\"Market\",\"Variable\",\"Type\",\"Unit\",\"T0\", day_col, \"shock_std\",\"Comment\"]\n",
    "    keep = [c for c in keep if c in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# =======================\n",
    "# Chargement exposures + QC\n",
    "# =======================\n",
    "def load_exposures(path_excel: str|Path, return_qc: bool = True):\n",
    "    \"\"\"Charge le fichier d'expositions, nettoie et met des valeurs par d√©faut pour √©viter les NaN.\"\"\"\n",
    "    df = pd.read_csv(path_excel,sep=';',decimal='.')\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    qc = {\"path\": str(path_excel)}\n",
    "    qc[\"rows_raw\"] = int(df.shape[0])\n",
    "\n",
    "    # Convertir en num√©rique les colonnes cl√©s si elles existent\n",
    "    num_cols = [\n",
    "        \"TV\",\"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\n",
    "        \"RateDelta1bp\",\"RateVega\",\"SpreadDelta1bp\",\"CreditVega\",\n",
    "        \"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\",\n",
    "        \"Nominal\",\"TVPercent\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Enlever les lignes agr√©g√©es 'TOTAL' si jamais elles existent\n",
    "    removed_total = 0\n",
    "    if \"AssetID\" in df.columns:\n",
    "        mask_total = df[\"AssetID\"].astype(str).str.upper().eq(\"TOTAL\")\n",
    "        removed_total = int(mask_total.sum())\n",
    "        df = df[~mask_total].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    qc[\"rows_removed_total\"] = removed_total\n",
    "    qc[\"rows_after_filter\"] = int(df.shape[0])\n",
    "\n",
    "    mask_0 = df['Nominal'] == 0\n",
    "    removed_n_0 = int(mask_0.sum())\n",
    "    df = df[~mask_0].copy()\n",
    "\n",
    "    qc['row_removes_notional_null'] = removed_n_0\n",
    "\n",
    "    # Sensi NaN -> 0 (√©vite les effets NaN)\n",
    "    sensi_zero = [\n",
    "        \"RateDelta1bp\",\"SpreadDelta1bp\",\"InflationDelta1bp\",\n",
    "        \"EquityDelta\",\"FXDelta\",\"RateVega\",\"CreditVega\",\"EquityVega\",\"FXVega\",\"EquityGamma\"\n",
    "    ]\n",
    "    qc[\"filled_zero\"] = {}\n",
    "    for c in sensi_zero:\n",
    "        if c in df.columns:\n",
    "            n = int(df[c].isna().sum())\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "            qc[\"filled_zero\"][c] = n\n",
    "\n",
    "    # Duration NaN -> 0\n",
    "    if \"Duration\" in df.columns:\n",
    "        qc[\"duration_filled_zero\"] = int(df[\"Duration\"].isna().sum())\n",
    "        df[\"Duration\"] = df[\"Duration\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"duration_missing_col\"] = True\n",
    "        df[\"Duration\"] = 0.0\n",
    "\n",
    "    # DollarRateConvexity1pc : NaN -> 0\n",
    "    if \"DollarRateConvexity1pc\" in df.columns:\n",
    "        qc[\"dollarconvexity_filled_zero\"] = int(df[\"DollarRateConvexity1pc\"].isna().sum())\n",
    "        df[\"DollarRateConvexity1pc\"] = df[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"dollarconvexity_missing_col\"] = True\n",
    "        df[\"DollarRateConvexity1pc\"] = 0.0\n",
    "\n",
    "    return (df, qc) if return_qc else df\n",
    "\n",
    "def print_qc_report(qc: dict):\n",
    "    \"\"\"Affiche un petit rapport de nettoyage des expositions.\"\"\"\n",
    "    print(f\"üìÑ Fichier : {qc.get('path','')}\")\n",
    "    print(f\"üì¶ Lignes brutes : {qc['rows_raw']}\")\n",
    "    print(f\"üßπ Lignes 'TOTAL' supprim√©es : {qc['rows_removed_total']}\")\n",
    "    print(f\"‚úÖ Lignes apr√®s filtre : {qc['rows_after_filter']}\")\n",
    "    print(\"\\nüîß NaN ‚Üí 0 (sensis) :\")\n",
    "    for k, v in qc[\"filled_zero\"].items():\n",
    "        print(f\"  - {k:<22}: {v}\")\n",
    "    if \"duration_filled_zero\" in qc:\n",
    "        print(f\"\\n‚è±  Duration NaN ‚Üí 0 : {qc['duration_filled_zero']}\")\n",
    "    if qc.get(\"duration_missing_col\"):\n",
    "        print(\"‚ö†Ô∏è  Colonne 'Duration' manquante ‚Üí cr√©√©e √† 0\")\n",
    "    if qc.get(\"dollarconvexity_missing_col\"):\n",
    "        print(\"‚ö†Ô∏è  Colonne 'DollarRateConvexity1pc' manquante ‚Üí cr√©√©e\")\n",
    "    if \"dollarconvexity_filled_zero\" in qc:\n",
    "        print(f\"üìê DollarRateConvexity1pc NaN ‚Üí 0 : {qc['dollarconvexity_filled_zero']}\")\n",
    "        \n",
    "def load_counterparty_mapping(path_excel: str|Path, id_col='FREE_TEXT_1', cp_col='CP') -> pd.DataFrame:\n",
    "    \"\"\"Lit le fichier Trioptima et renvoie un mapping Identifier‚ÜíCounterparty.\"\"\"\n",
    "    df = pd.read_excel(path_excel)\n",
    "    df = _clean_cols(df)\n",
    "    if id_col not in df.columns or cp_col not in df.columns:\n",
    "        raise KeyError(f\"Colonnes '{id_col}' ou '{cp_col}' manquantes dans {path_excel}\")\n",
    "    return (df[[id_col, cp_col]]\n",
    "            .dropna(subset=[id_col])\n",
    "            .rename(columns={id_col: 'Identifier', cp_col: 'Counterparty'})\n",
    "            .drop_duplicates('Identifier'))\n",
    "\n",
    "# =======================\n",
    "# √âtape \"un jour\" ‚Äî base = exposures, cl√© = Identifier\n",
    "# =======================\n",
    "def day_step_apply(\n",
    "    exposures: pd.DataFrame,\n",
    "    merged_mapping: pd.DataFrame,\n",
    "    day_col: str = \"Day 1\",\n",
    "    include_other_inflation_swap: bool = True,\n",
    "    update_duration: bool = True,   # Duration_next = Duration_t + DollarRateConvexity1pc/100 √ó shock_rates_dec\n",
    "    update_tv: bool = True,         # TV_day = TV + TotalEffect\n",
    "    return_pivot: bool = True,\n",
    "    key_col: str = \"Identifier\",    # cl√© de jointure\n",
    "    port_col: str = \"Portfolio\",    # colonne de portefeuille\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique les chocs d'un jour et renvoie:\n",
    "      - detailed       : lignes (Portfolio √ó Identifier √ó Market √ó Variable) avec Effect/Method (+ ISIN si dispo)\n",
    "      - per_id         : agr√©gat par Portefeuille/Identifier (ISIN=first), TV_day, (Duration_next si update_duration)\n",
    "      - exposures_next : copie de exposures avec TV/Duration mises √† jour (selon flags)\n",
    "      - pivot          : (optionnel) large des Effects ('Market :: Variable'), index = (Portefeuille, Identifier)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Chocs du jour (standardis√©s en d√©cimal)\n",
    "    shocks_day = build_daily_shocks(merged_mapping, day_col=day_col)  # contient Identifier, ISIN (POS), Market, Variable, shock_std...\n",
    "    shocks_day = shocks_day.drop_duplicates(subset=[key_col, \"Market\", \"Variable\"])\n",
    "    # 2) Merge en LEFT depuis exposures (la base de calcul) par Identifier et ajoute le Portefeuille\n",
    "    cols_needed = [\n",
    "        key_col, port_col, \"AssetClass\",\"Country\",\"AssetID\",\"Nominal\",\"TVPercent\",\"TV\",\n",
    "        \"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\"RateDelta1bp\",\"RateVega\",\n",
    "        \"SpreadDelta1bp\",\"CreditVega\",\"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\", \"Counterparty\"\n",
    "    ]\n",
    "    cols_needed = [c for c in cols_needed if c in exposures.columns]\n",
    "    base = exposures[cols_needed].copy()\n",
    "    if key_col not in base.columns or key_col not in shocks_day.columns:\n",
    "        raise KeyError(f\"Cl√© '{key_col}' absente de exposures ou du mapping de sc√©narios.\")\n",
    "\n",
    "    df = base.merge(shocks_day, on=key_col, how=\"left\")  # garde les instruments sans mapping (effet=0)\n",
    "\n",
    "    # 3) Calcul des effets par ligne (Portfolio √ó Identifier √ó Market √ó Variable)\n",
    "    effects, methods = [], []\n",
    "    for _, r in df.iterrows():\n",
    "        market_raw = r.get(\"Market\")\n",
    "        if isinstance(market_raw, str):\n",
    "            market = market_raw.lower()\n",
    "        elif pd.notna(market_raw):\n",
    "            market = str(market_raw).lower()\n",
    "        else:\n",
    "            market = \"\"\n",
    "        variable = (r.get(\"Variable\") or \"\")\n",
    "        unit     = r.get(\"Unit\")\n",
    "        shock_std = r.get(\"shock_std\", np.nan)\n",
    "\n",
    "        shock_bps = _to_bps(shock_std, unit)  # pour PV01/CS01/Infl01 (en bps num√©riques)\n",
    "        shock_dec = shock_std                 # d√©cimal (ex: +50 bps -> +0.005)\n",
    "\n",
    "        effect = np.nan\n",
    "        method = None\n",
    "\n",
    "        if market == \"equity\":\n",
    "            eq_delta = r.get(\"EquityDelta\", np.nan)\n",
    "            if pd.notna(eq_delta) and pd.notna(shock_dec):\n",
    "                effect = eq_delta * shock_dec\n",
    "                method = \"EquityDelta √ó shock_dec\"\n",
    "\n",
    "        elif market == \"interest rates\":\n",
    "            if pd.notna(r.get(\"RateDelta1bp\")) and pd.notna(shock_bps):\n",
    "                effect = r[\"RateDelta1bp\"] * shock_bps\n",
    "                method = \"RateDelta1bp √ó shock_bps\"\n",
    "\n",
    "        elif \"spread\" in market:  # couvre Gov Spreads / Corp Spreads (peu importe la casse)\n",
    "            sp01 = r.get(\"SpreadDelta1bp\", np.nan)\n",
    "            if pd.notna(sp01) and pd.notna(shock_bps):\n",
    "                effect = sp01 * shock_bps\n",
    "                method = \"SpreadDelta1bp √ó shock_bps\"\n",
    "\n",
    "        elif market == \"fx\":\n",
    "            fx_delta = r.get(\"FXDelta\", np.nan)\n",
    "            if pd.notna(fx_delta) and pd.notna(shock_dec):\n",
    "                effect = fx_delta * shock_dec\n",
    "                method = \"FXDelta √ó shock_dec\"\n",
    "\n",
    "        elif market == \"other\":\n",
    "            # Cas sp√©cifique demand√©: Other + Variable = Inflation Swap\n",
    "            if isinstance(variable, str) and \"inflation swap\" in variable.lower():\n",
    "                infl01 = r.get(\"InflationDelta1bp\", np.nan)\n",
    "                if pd.notna(infl01) and pd.notna(shock_bps):\n",
    "                    effect = infl01 * shock_bps\n",
    "                    method = \"InflationDelta1bp √ó shock_bps (Other/Inflation Swap)\"\n",
    "\n",
    "        effects.append(effect)\n",
    "        methods.append(method)\n",
    "\n",
    "    df[\"Effect\"] = effects\n",
    "    df[\"Method\"] = methods\n",
    "    df[\"Effect\"] = df[\"Effect\"].fillna(0.0)  # pas de mapping -> effet 0\n",
    "\n",
    "    # 4) Agr√©gat par Portefeuille/Identifier (on conserve ISIN = first pour info)\n",
    "    agg = {\n",
    "        \"TV\": (\"TV\",\"first\"),\n",
    "        \"Duration_t\": (\"Duration\",\"first\"),\n",
    "        \"DollarRateConvexity1pc\": (\"DollarRateConvexity1pc\",\"first\"),\n",
    "        \"TotalEffect\": (\"Effect\",\"sum\"),\n",
    "        \"Counterparty\":(\"Counterparty\",\"first\")\n",
    "    }\n",
    "    if \"ISIN\" in df.columns:\n",
    "        agg[\"ISIN\"] = (\"ISIN\",\"first\")\n",
    "\n",
    "    if \"AssetClass\" in df.columns:\n",
    "        agg[\"AssetClass\"] = (\"AssetClass\",\"first\")\n",
    "\n",
    "    \n",
    "\n",
    "    per_id = df.groupby([port_col, key_col], as_index=False).agg(**agg) \n",
    "    per_id[\"TV_day\"] = per_id[\"TV\"] + per_id[\"TotalEffect\"]\n",
    "\n",
    "    # 5) (option) mise √† jour de la Duration : Duration_next = Duration_t + DollarRateConvexity1pc/100 √ó (‚àëchoc_rates_dec)\n",
    "    if update_duration:\n",
    "        rates_mask = df[\"Market\"].fillna(\"\").str.lower().eq(\"interest rates\")\n",
    "        rates_choc_dec = (df.loc[rates_mask]\n",
    "                            .groupby([port_col, key_col], as_index=False)[\"shock_std\"]\n",
    "                            .sum()\n",
    "                            .rename(columns={\"shock_std\":\"shock_rates_dec\"}))\n",
    "        per_id = per_id.merge(rates_choc_dec, on=[port_col, key_col], how=\"left\")\n",
    "        per_id[\"shock_rates_dec\"] = per_id[\"shock_rates_dec\"].fillna(0.0)\n",
    "        per_id[\"DollarRateConvexity1pc\"] = per_id[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "        per_id[\"Duration_t\"]      = per_id[\"Duration_t\"].fillna(0.0)\n",
    "        per_id[\"Duration_next\"]   = per_id[\"Duration_t\"] + (per_id[\"DollarRateConvexity1pc\"] / 100.0) * per_id[\"shock_rates_dec\"]\n",
    "\n",
    "    # 6) Construire exposures_next (mise √† jour TV/Duration par (Portefeuille, Identifier))\n",
    "\n",
    "\n",
    "    exposures_next = exposures.copy()\n",
    "    merge_cols = [port_col, key_col]\n",
    "    update_cols = merge_cols + [\"TV_day\"]\n",
    "    if update_duration:\n",
    "        update_cols.append(\"Duration_next\")\n",
    "        \n",
    "    exposures_next = exposures_next.merge(\n",
    "        per_id[update_cols],\n",
    "        on=merge_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    exposures_next[\"TV\"] = exposures_next[\"TV_day\"].fillna(exposures_next[\"TV\"])\n",
    "    if update_duration and \"Duration_next\" in exposures_next.columns:\n",
    "        exposures_next[\"Duration\"] = exposures_next[\"Duration_next\"].fillna(exposures_next[\"Duration\"])\n",
    "        \n",
    "    exposures_next = exposures_next.drop(columns=[c for c in [\"TV_day\", \"Duration_next\"] if c in exposures_next.columns])\n",
    "\n",
    "\n",
    "    return df, per_id, exposures_next\n",
    "\n",
    "\n",
    "# ... puis Day 3, Day 4, Day 5, Day 10 de la m√™me mani√®re.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pv_after_day1(exposures_next: pd.DataFrame,\n",
    "                          future_classes=(\"Bond Future\", \"Equity Index Future\", \"FX Future\"),\n",
    "                          cp_col=\"Counterparty\", port_col=\"Portfolio\") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Aggregate TV after Day 1 and reset PV for selected futures classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposures_next : pd.DataFrame\n",
    "        DataFrame returned by ``day_step_apply`` for Day 1.\n",
    "    future_classes : iterable[str]\n",
    "        Asset classes whose TV must be summed then reset to 0.\n",
    "    cp_col, port_col : str\n",
    "        Column names for counterparty and portfolio.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    updated : pd.DataFrame\n",
    "        ``exposures_next`` with TV set to 0 for the specified futures classes.\n",
    "    futures_tv : pd.DataFrame\n",
    "        Sum of TV by AssetClass before reset (column ``TV_before_reset``).\n",
    "    cp_port_tv : pd.DataFrame\n",
    "        Sum of TV by Counterparty and Portfolio for remaining asset classes before reset (column ``TV_before_reset``).\n",
    "    \"\"\"\n",
    "    df = exposures_next.copy()\n",
    "    if \"AssetClass\" not in df.columns:\n",
    "        raise KeyError(\"Column 'AssetClass' missing in exposures_next\")\n",
    "    mask = df[\"AssetClass\"].isin(future_classes)\n",
    "    futures_tv = (\n",
    "        df.loc[mask]\n",
    "          .groupby(\"AssetClass\", as_index=False)[\"TV\"].sum()\n",
    "          .rename(columns={\"TV\": \"TV_before_reset\"})\n",
    "          .sort_values(\"AssetClass\")\n",
    "    )\n",
    "    other_mask = ~mask\n",
    "    group_cols = [c for c in (cp_col, port_col) if c in df.columns]\n",
    "    cp_port_tv = (\n",
    "        df.loc[other_mask]\n",
    "          .groupby(group_cols, as_index=False)[\"TV\"].sum()\n",
    "          .rename(columns={\"TV\": \"TV_before_collat\"})\n",
    "          .sort_values(group_cols)\n",
    "        if len(group_cols) == 2 else pd.DataFrame(columns=group_cols + [\"TV_before_collat\"])\n",
    "    )\n",
    "    df.loc[mask, \"TV\"] = 0.0\n",
    "    return df, futures_tv, cp_port_tv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Fichier : C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\n",
      "üì¶ Lignes brutes : 222\n",
      "üßπ Lignes 'TOTAL' supprim√©es : 2\n",
      "‚úÖ Lignes apr√®s filtre : 220\n",
      "\n",
      "üîß NaN ‚Üí 0 (sensis) :\n",
      "  - RateDelta1bp          : 1\n",
      "  - SpreadDelta1bp        : 40\n",
      "  - InflationDelta1bp     : 191\n",
      "  - FXDelta               : 181\n",
      "\n",
      "‚è±  Duration NaN ‚Üí 0 : 10\n",
      "üìê DollarRateConvexity1pc NaN ‚Üí 0 : 24\n",
      "TV   -4.360630e+07\n",
      "dtype: float64\n",
      "     Portfolio     Identifier            TV\n",
      "81      900200  CDSB100001671  3.224981e+05\n",
      "93      900200  IRSB100009460 -4.501091e+07\n",
      "139     981017   TRS100000144  9.460860e+05\n",
      "140     981017   TRS100000147  9.767983e+05\n",
      "141     981017   TRS100000148  5.003811e+06\n",
      "182     981017     FXW1023898 -1.571135e+06\n",
      "183     981017     FXW1023898  1.562492e+06\n",
      "208     981017  IRSB100002012 -6.927264e+05\n",
      "210     981017  IRSB100008421 -5.143210e+06\n",
      "TV avant remise √† z√©ro par AssetClass (futures):\n",
      "            AssetClass  TV_before_reset\n",
      "0          Bond Future     3.686943e+06\n",
      "1  Equity Index Future     0.000000e+00\n",
      "TV avant collat√©ral par Counterparty/Portfolio pour les autres classes d'actifs:\n",
      "   Counterparty  Portfolio  TV_before_collat\n",
      "0          BNPP     900200     -4.474831e+07\n",
      "1          BNPP     981017     -2.061516e+06\n",
      "2            CA     900200     -1.614555e+07\n",
      "3            CA     981017     -4.305859e+04\n",
      "4           CEP     900200      1.106920e+05\n",
      "5           CEP     981017     -6.289544e+06\n",
      "6         DBKAG     981017      5.076092e+05\n",
      "7          GIPB     900200     -2.628448e+05\n",
      "8          GIPB     981017     -2.253084e+05\n",
      "9          GSOH     900200     -1.552466e+06\n",
      "10        JPMSE     900200     -6.725968e+06\n",
      "11        JPMSE     981017     -1.105153e+06\n",
      "12        MSESE     900200      3.627375e+05\n",
      "13        MSESE     981017     -2.686071e+07\n",
      "14      NATIXIS     981017     -4.191402e+07\n",
      "15        SGCIB     900200     -1.642963e+05\n",
      "TV   -4.680983e+07\n",
      "dtype: float64\n",
      "     Portfolio     Identifier            TV\n",
      "81      900200  CDSB100001671  1.966255e+05\n",
      "93      900200  IRSB100009460 -4.494493e+07\n",
      "139     981017   TRS100000144  9.451451e+05\n",
      "140     981017   TRS100000147  9.742727e+05\n",
      "141     981017   TRS100000148  4.988177e+06\n",
      "182     981017     FXW1023898 -1.570742e+06\n",
      "183     981017     FXW1023898 -1.570742e+06\n",
      "208     981017  IRSB100002012 -6.874108e+05\n",
      "210     981017  IRSB100008421 -5.140216e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "c:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Charger mapping & sc√©narios\n",
    "pos_raw, scen_raw = load_mapping(EXCEL_MAPPING_PATH)\n",
    "scen_raw=scen_raw.iloc[:29,:]\n",
    "\n",
    "\n",
    "# 2) Charger exposures (greeks) + QC\n",
    "exposures, qc = load_exposures(EXPOSURES_PATH, return_qc=True)\n",
    "print_qc_report(qc)\n",
    "\n",
    "# Ajouter les contreparties depuis Trioptima\n",
    "cp_map = load_counterparty_mapping(TRIOPTIMA_PATH)\n",
    "exposures = exposures.merge(cp_map, on='Identifier', how='left')\n",
    "\n",
    "# Limiter POS au p√©rim√®tre d'exposures avant melt\n",
    "pos_subset = exposures[['Identifier','Counterparty']].merge(pos_raw, on='Identifier', how='left')\n",
    "# 3) Pr√©parer mapping restreint & sc√©narios\\n\",\n",
    "pos_long = melt_pos(pos_subset)\n",
    "scen     = prepare_scenarios(scen_raw)\n",
    "merged   = merge_pos_scen(pos_long, scen)          # mapping POS ‚Üî sc√©narios\n",
    "\n",
    "\n",
    "# 4) DAY 1\n",
    "d1_det, d1_id, exp_d2 = day_step_apply(\n",
    "    exposures=exposures,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 1\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Agr√©gation des TV et remise √† z√©ro pour certains futures\n",
    "exp_d2, tv_futures, tv_cp_port = process_pv_after_day1(exp_d2, port_col=\"Portfolio\")\n",
    "print(\"TV avant remise √† z√©ro par AssetClass (futures):\")\n",
    "print(tv_futures)\n",
    "print(\"TV avant collat√©ral par Counterparty/Portfolio pour les autres classes d'actifs:\")\n",
    "print(tv_cp_port)\n",
    "#  exp_d2 est ta base pour Day 2 (tu peux la MODIFIER maintenant)\n",
    "\n",
    "# 5) (EXEMPLE) Modifs entre Day 1 et Day 2\n",
    "# exp_d2.loc[exp_d2[\"Identifier\"]==\"ID_√Ä_VENDRE\",\"TV\"] *= 0.95\n",
    "# exp_d2 = exp_d2[exp_d2[\"Identifier\"]!=\"ID_A_SORTIR\"]\n",
    "# exp_d2.loc[exp_d2[\"Identifier\"]==\"NOUVEL_ID\",\"Duration\"] = 6.8\n",
    "# (si tu as recalcul√© des sensis overnight, √©crase les colonnes correspondantes dans exp_d2)\n",
    "\n",
    "# 6) DAY 2 (sur la base modifi√©e)\n",
    "d2_det, d2_id, exp_d3 = day_step_apply(\n",
    "    exposures=exp_d2,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 2\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
