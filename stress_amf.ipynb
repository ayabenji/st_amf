{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "# AMF Stress Test — workflow JOUR PAR JOUR (base = exposures)\n",
    "# ================================================================\n",
    "# Ce script :\n",
    "# 1) charge le mapping (POS + scenario_paths),\n",
    "# 2) charge 'exposures' (greeks) + nettoie/contrôle qualité (QC),\n",
    "# 3) restreint les scénarios au périmètre d'exposures (par Identifier),\n",
    "# 4) applique les chocs pour un jour donné (day_step_apply),\n",
    "# 5) renvoie un 'exposures_next' prêt à être MODIFIÉ avant le jour suivant.\n",
    "# ------------------------------------------------\n",
    "# CLÉ DE MERGE / AGRÉGATION = Identifier  (ISIN conservé pour info)\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collateral_management import process_pv_after_day_1, roll_balance_for_next_day\n",
    "\n",
    "# =======================\n",
    "# CONFIG — à adapter\n",
    "# =======================\n",
    "EXCEL_MAPPING_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\Matrices correspondance_AB.xlsx\"\n",
    "SHEET_POS = \"Test_Aya\"   # entêtes ligne 2 -> header=1\n",
    "SHEET_SCEN =  \"scenario_paths\"\n",
    "POS_HEADER_ROW = 1               # 0-based\n",
    "\n",
    "EXPOSURES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\"\n",
    "TRIOPTIMA_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\search_groupama-am_2025-03-31.xlsx\"\n",
    "COLLATERAL_BALANCES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\Collat_Cash_MTM_LU_20250401.csv\" \n",
    "COLLATERAL_BALANCE_DAY_COL = \"Balance J\"\n",
    "COLLATERAL_BALANCE_PREV_COL = \"Balance J-1\"\n",
    "COLLATERAL_THRESHOLD_COL = \"Seuil déclenchement\"\n",
    "\n",
    "# Jours possibles (doivent exister dans scenario_paths)\n",
    "DAYS = [\"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "# Règles de méthode\n",
    "INCLUDE_OTHER_INFLATION_SWAP = True\n",
    "\n",
    "# =======================\n",
    "# Utils colonnes / texte\n",
    "# =======================\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie des colonnes Excel (espaces, 'Unnamed', points).\"\"\"\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c is None or (isinstance(c, str) and c.lower().startswith(\"unnamed\")):\n",
    "            new_cols.append(None); continue\n",
    "        s = str(c).strip().replace(\"\\u00A0\", \" \")\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace(\". \", \" \").replace(\".\", \" \")\n",
    "        new_cols.append(s)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def _norm_str(x):\n",
    "    \"\"\"Normalise légèrement une chaîne (pour Market / Variable).\"\"\"\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().replace(\"\\u00A0\", \" \")\n",
    "    s = \" \".join(s.split()).replace(\". \", \" \").replace(\".\", \" \")\n",
    "    return s\n",
    "\n",
    "# =======================\n",
    "# Chargement mapping (POS + scen)\n",
    "# =======================\n",
    "POS_BASE_COLS = [\n",
    "    \"Identifier\",\"ISIN\",\"Counterparty\",\"Description\",\"Currency\",\"AssetType\",\"Sector1\",\"Seniority\",\n",
    "    \"CompositeBroadRating\",\"MaturityDate\",\"Maturity\",\"Maturity Band\",\"EffectiveMaturityDate\",\n",
    "    \"LiquidityScore\",\"Country\",\"{Class_Rating}\"\n",
    "]\n",
    "POS_MV_PAIRS = [(f\"Market {i}\", f\"Variable {i}\") for i in range(1, 7)]\n",
    "SCEN_BASE_COLS = [\"Market\",\"Variable\",\"Comment\",\"Type\",\"Unit\",\"T0\",\n",
    "                  \"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "def load_mapping(path_excel: str|Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lit la feuille POS et scenario_paths.\"\"\"\n",
    "    xls = pd.ExcelFile(path_excel)\n",
    "    pos  = pd.read_excel(xls, SHEET_POS, header=POS_HEADER_ROW)\n",
    "    scen = pd.read_excel(xls, SHEET_SCEN)\n",
    "    pos  = _clean_cols(pos)\n",
    "    scen = _clean_cols(scen)\n",
    "    return pos, scen\n",
    "\n",
    "def melt_pos(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforme POS en format long : 1 ligne par (Market k, Variable k) non vide.\"\"\"\n",
    "    pos = pos.copy()\n",
    "    for mk, vk in POS_MV_PAIRS:\n",
    "        if mk in pos.columns: pos[mk] = pos[mk].apply(_norm_str)\n",
    "        if vk in pos.columns: pos[vk] = pos[vk].apply(_norm_str)\n",
    "\n",
    "    base_cols = [c for c in POS_BASE_COLS if c in pos.columns]\n",
    "    mv_pairs_present = [(mk,vk) for mk,vk in POS_MV_PAIRS if mk in pos.columns and vk in pos.columns]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in pos.iterrows():\n",
    "        base = {c: row.get(c, np.nan) for c in base_cols}\n",
    "        for mk, vk in mv_pairs_present:\n",
    "            market, variable = row[mk], row[vk]\n",
    "            if pd.notna(market) and str(market) != \"\":\n",
    "                rows.append({**base, \"Market\": market, \"Variable\": (variable if pd.notna(variable) else np.nan)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"Identifier\" not in out.columns: out[\"Identifier\"] = np.arange(len(out))\n",
    "    return out\n",
    "\n",
    "def prepare_scenarios(scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie la table scenario_paths & conserve les colonnes utiles.\"\"\"\n",
    "    scen = scen.copy()\n",
    "    scen = scen[[c for c in SCEN_BASE_COLS if c in scen.columns]]\n",
    "    scen[\"Market\"] = scen[\"Market\"].apply(_norm_str)\n",
    "    if \"Variable\" in scen.columns: scen[\"Variable\"] = scen[\"Variable\"].apply(_norm_str)\n",
    "    if \"Type\" in scen.columns:     scen[\"Type\"]     = scen[\"Type\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    if \"Unit\" in scen.columns:     scen[\"Unit\"]     = scen[\"Unit\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    return scen\n",
    "\n",
    "def merge_pos_scen(pos_long: pd.DataFrame, scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Relie POS (long) aux scénarios : jointure (Market,Variable) ; si Variable vide -> jointure sur Market seul.\"\"\"\n",
    "    left_mv = pos_long.dropna(subset=[\"Market\",\"Variable\"]) if \"Variable\" in pos_long.columns else pos_long.copy()\n",
    "    mv_merge = left_mv.merge(scen, on=[\"Market\",\"Variable\"], how=\"left\")\n",
    "    if \"Variable\" in pos_long.columns: left_m = pos_long[pos_long[\"Variable\"].isna()].copy()\n",
    "    else:                               left_m = pd.DataFrame(columns=pos_long.columns)\n",
    "    if not left_m.empty:\n",
    "        m_merge = left_m.merge(scen.drop(columns=[\"Variable\"], errors=\"ignore\"), on=\"Market\", how=\"left\")\n",
    "        return pd.concat([mv_merge, m_merge], ignore_index=True)\n",
    "    return mv_merge\n",
    "\n",
    "def available_days(scen: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Retourne la liste des colonnes 'Day n' disponibles.\"\"\"\n",
    "    return [c for c in scen.columns if isinstance(c, str) and c.lower().startswith(\"day\")]\n",
    "\n",
    "# =======================\n",
    "# Limiter les scénarios au périmètre d'exposures (clé = Identifier)\n",
    "# =======================\n",
    "def restrict_scenarios_to_exposures(merged_mapping: pd.DataFrame,\n",
    "                                    exposures: pd.DataFrame,\n",
    "                                    key_col: str = \"Identifier\") -> pd.DataFrame:\n",
    "    \"\"\"Garde uniquement les lignes de mapping dont la clé existe dans exposures.\"\"\"\n",
    "    if key_col not in merged_mapping.columns or key_col not in exposures.columns:\n",
    "        return merged_mapping\n",
    "    keys = exposures[key_col].astype(str).unique()\n",
    "    return merged_mapping[merged_mapping[key_col].astype(str).isin(keys)].copy()\n",
    "\n",
    "# =======================\n",
    "# Standardisation des chocs\n",
    "# =======================\n",
    "def standardize_shock(value, unit: str|None) -> float|None:\n",
    "    \"\"\"Choc standardisé en décimal: 50 bps -> 0.005 ; -10% -> -0.10 ; 2 p.p -> 0.02.\"\"\"\n",
    "    if pd.isna(value): return None\n",
    "    try: val = float(value)\n",
    "    except: return None\n",
    "    if unit is None: return val\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return val / 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\"]:\n",
    "        return val / 100.0\n",
    "    return val\n",
    "\n",
    "def _to_bps(shock_std: float, unit: str|None) -> float:\n",
    "    \"\"\"Convertit un choc standardisé en bps numériques si besoin (pour PV01 / CS01 / Infl01).\"\"\"\n",
    "    if shock_std is None or pd.isna(shock_std) or unit is None: return np.nan\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return shock_std * 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\",\"pc\"]:\n",
    "        return shock_std * 10_000.0\n",
    "    return np.nan\n",
    "\n",
    "def build_daily_shocks(merged: pd.DataFrame, day_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Prépare les chocs d'un jour (inclut Identifier & ISIN si présents).\"\"\"\n",
    "    if day_col not in merged.columns:\n",
    "        raise ValueError(f\"Jour '{day_col}' introuvable. Jours dispo: {available_days(merged)}\")\n",
    "    out = merged.copy()\n",
    "    out[\"shock_raw\"] = out[day_col]\n",
    "    out[\"shock_std\"] = [standardize_shock(v, u) for v, u in zip(out[\"shock_raw\"], out.get(\"Unit\", pd.Series([None]*len(out))))]\n",
    "\n",
    "    keep = [\"Identifier\",\"ISIN\",\"Market\",\"Variable\",\"Type\",\"Unit\",\"T0\", day_col, \"shock_std\",\"Comment\"]\n",
    "    keep = [c for c in keep if c in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# =======================\n",
    "# Chargement exposures + QC\n",
    "# =======================\n",
    "def load_exposures(path_excel: str|Path, return_qc: bool = True):\n",
    "    \"\"\"Charge le fichier d'expositions, nettoie et met des valeurs par défaut pour éviter les NaN.\"\"\"\n",
    "    df = pd.read_csv(path_excel,sep=';',decimal='.')\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    qc = {\"path\": str(path_excel)}\n",
    "    qc[\"rows_raw\"] = int(df.shape[0])\n",
    "\n",
    "    # Convertir en numérique les colonnes clés si elles existent\n",
    "    num_cols = [\n",
    "        \"TV\",\"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\n",
    "        \"RateDelta1bp\",\"RateVega\",\"SpreadDelta1bp\",\"CreditVega\",\n",
    "        \"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\",\n",
    "        \"Nominal\",\"TVPercent\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Enlever les lignes agrégées 'TOTAL' si jamais elles existent\n",
    "    removed_total = 0\n",
    "    if \"AssetID\" in df.columns:\n",
    "        mask_total = df[\"AssetID\"].astype(str).str.upper().eq(\"TOTAL\")\n",
    "        removed_total = int(mask_total.sum())\n",
    "        df = df[~mask_total].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    qc[\"rows_removed_total\"] = removed_total\n",
    "    qc[\"rows_after_filter\"] = int(df.shape[0])\n",
    "\n",
    "    mask_0 = df['Nominal'] == 0\n",
    "    removed_n_0 = int(mask_0.sum())\n",
    "    df = df[~mask_0].copy()\n",
    "\n",
    "    qc['row_removes_notional_null'] = removed_n_0\n",
    "\n",
    "    # Sensi NaN -> 0 (évite les effets NaN)\n",
    "    sensi_zero = [\n",
    "        \"RateDelta1bp\",\"SpreadDelta1bp\",\"InflationDelta1bp\",\n",
    "        \"EquityDelta\",\"FXDelta\",\"RateVega\",\"CreditVega\",\"EquityVega\",\"FXVega\",\"EquityGamma\"\n",
    "    ]\n",
    "    qc[\"filled_zero\"] = {}\n",
    "    for c in sensi_zero:\n",
    "        if c in df.columns:\n",
    "            n = int(df[c].isna().sum())\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "            qc[\"filled_zero\"][c] = n\n",
    "\n",
    "    # Duration NaN -> 0\n",
    "    if \"Duration\" in df.columns:\n",
    "        qc[\"duration_filled_zero\"] = int(df[\"Duration\"].isna().sum())\n",
    "        df[\"Duration\"] = df[\"Duration\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"duration_missing_col\"] = True\n",
    "        df[\"Duration\"] = 0.0\n",
    "\n",
    "    # DollarRateConvexity1pc : NaN -> 0\n",
    "    if \"DollarRateConvexity1pc\" in df.columns:\n",
    "        qc[\"dollarconvexity_filled_zero\"] = int(df[\"DollarRateConvexity1pc\"].isna().sum())\n",
    "        df[\"DollarRateConvexity1pc\"] = df[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"dollarconvexity_missing_col\"] = True\n",
    "        df[\"DollarRateConvexity1pc\"] = 0.0\n",
    "\n",
    "    return (df, qc) if return_qc else df\n",
    "\n",
    "def print_qc_report(qc: dict):\n",
    "    \"\"\"Affiche un petit rapport de nettoyage des expositions.\"\"\"\n",
    "    print(f\"📄 Fichier : {qc.get('path','')}\")\n",
    "    print(f\"📦 Lignes brutes : {qc['rows_raw']}\")\n",
    "    print(f\"🧹 Lignes 'TOTAL' supprimées : {qc['rows_removed_total']}\")\n",
    "    print(f\"✅ Lignes après filtre : {qc['rows_after_filter']}\")\n",
    "    print(\"🔧 NaN → 0 (sensis) :\")\n",
    "    for k, v in qc[\"filled_zero\"].items():\n",
    "        print(f\"  - {k:<22}: {v}\")\n",
    "    if \"duration_filled_zero\" in qc:\n",
    "        print(f\"\\n⏱  Duration NaN → 0 : {qc['duration_filled_zero']}\")\n",
    "    if qc.get(\"duration_missing_col\"):\n",
    "        print(\"⚠️  Colonne 'Duration' manquante → créée à 0\")\n",
    "    if qc.get(\"dollarconvexity_missing_col\"):\n",
    "        print(\"⚠️  Colonne 'DollarRateConvexity1pc' manquante → créée\")\n",
    "    if \"dollarconvexity_filled_zero\" in qc:\n",
    "        print(f\"📐 DollarRateConvexity1pc NaN → 0 : {qc['dollarconvexity_filled_zero']}\")\n",
    "        \n",
    "def load_counterparty_mapping(path_excel: str|Path, id_col='FREE_TEXT_1', cp_col='CP') -> pd.DataFrame:\n",
    "    \"\"\"Lit le fichier Trioptima et renvoie un mapping Identifier→Counterparty.\"\"\"\n",
    "    df = pd.read_excel(path_excel)\n",
    "    df = _clean_cols(df)\n",
    "    if id_col not in df.columns or cp_col not in df.columns:\n",
    "        raise KeyError(f\"Colonnes '{id_col}' ou '{cp_col}' manquantes dans {path_excel}\")\n",
    "    return (df[[id_col, cp_col]]\n",
    "            .dropna(subset=[id_col])\n",
    "            .rename(columns={id_col: 'Identifier', cp_col: 'Counterparty'})\n",
    "            .drop_duplicates('Identifier'))\n",
    "\n",
    "# =======================\n",
    "# Étape \"un jour\" — base = exposures, clé = Identifier\n",
    "# =======================\n",
    "def day_step_apply(\n",
    "    exposures: pd.DataFrame,\n",
    "    merged_mapping: pd.DataFrame,\n",
    "    day_col: str = \"Day 1\",\n",
    "    include_other_inflation_swap: bool = True,\n",
    "    update_duration: bool = True,   # Duration_next = Duration_t + DollarRateConvexity1pc/100 × shock_rates_dec\n",
    "    update_tv: bool = True,         # TV_day = TV + TotalEffect\n",
    "    return_pivot: bool = True,\n",
    "    key_col: str = \"Identifier\",    # clé de jointure\n",
    "    port_col: str = \"Portfolio\",    # colonne de portefeuille\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique les chocs d'un jour et renvoie:\n",
    "      - detailed       : lignes (Portfolio × Identifier × Market × Variable) avec Effect/Method (+ ISIN si dispo)\n",
    "      - per_id         : agrégat par Portefeuille/Identifier (ISIN=first), TV_day, (Duration_next si update_duration)\n",
    "      - exposures_next : copie de exposures avec TV/Duration mises à jour (selon flags)\n",
    "      - pivot          : (optionnel) large des Effects ('Market :: Variable'), index = (Portefeuille, Identifier)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Chocs du jour (standardisés en décimal)\n",
    "    shocks_day = build_daily_shocks(merged_mapping, day_col=day_col)  # contient Identifier, ISIN (POS), Market, Variable, shock_std...\n",
    "    shocks_day = shocks_day.drop_duplicates(subset=[key_col, \"Market\", \"Variable\"])\n",
    "    # 2) Merge en LEFT depuis exposures (la base de calcul) par Identifier et ajoute le Portefeuille\n",
    "    cols_needed = [\n",
    "        key_col, port_col, \"AssetClass\",\"Country\",\"AssetID\",\"Nominal\",\"TVPercent\",\"TV\",\n",
    "        \"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\"RateDelta1bp\",\"RateVega\",\n",
    "        \"SpreadDelta1bp\",\"CreditVega\",\"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\", \"Counterparty\"\n",
    "    ]\n",
    "    cols_needed = [c for c in cols_needed if c in exposures.columns]\n",
    "    base = exposures[cols_needed].copy()\n",
    "    if key_col not in base.columns or key_col not in shocks_day.columns:\n",
    "        raise KeyError(f\"Clé '{key_col}' absente de exposures ou du mapping de scénarios.\")\n",
    "\n",
    "    df = base.merge(shocks_day, on=key_col, how=\"left\")  # garde les instruments sans mapping (effet=0)\n",
    "\n",
    "    # 3) Calcul des effets par ligne (Portfolio × Identifier × Market × Variable)\n",
    "    effects, methods = [], []\n",
    "    for _, r in df.iterrows():\n",
    "        market_raw = r.get(\"Market\")\n",
    "        if isinstance(market_raw, str):\n",
    "            market = market_raw.lower()\n",
    "        elif pd.notna(market_raw):\n",
    "            market = str(market_raw).lower()\n",
    "        else:\n",
    "            market = \"\"\n",
    "        variable = (r.get(\"Variable\") or \"\")\n",
    "        unit     = r.get(\"Unit\")\n",
    "        shock_std = r.get(\"shock_std\", np.nan)\n",
    "\n",
    "        shock_bps = _to_bps(shock_std, unit)  # pour PV01/CS01/Infl01 (en bps numériques)\n",
    "        shock_dec = shock_std                 # décimal (ex: +50 bps -> +0.005)\n",
    "\n",
    "        effect = np.nan\n",
    "        method = None\n",
    "\n",
    "        if market == \"equity\":\n",
    "            eq_delta = r.get(\"EquityDelta\", np.nan)\n",
    "            if pd.notna(eq_delta) and pd.notna(shock_dec):\n",
    "                effect = eq_delta * shock_dec\n",
    "                method = \"EquityDelta × shock_dec\"\n",
    "\n",
    "        elif market == \"interest rates\":\n",
    "            if pd.notna(r.get(\"RateDelta1bp\")) and pd.notna(shock_bps):\n",
    "                effect = r[\"RateDelta1bp\"] * shock_bps\n",
    "                method = \"RateDelta1bp × shock_bps\"\n",
    "\n",
    "        elif \"spread\" in market:  # couvre Gov Spreads / Corp Spreads (peu importe la casse)\n",
    "            sp01 = r.get(\"SpreadDelta1bp\", np.nan)\n",
    "            if pd.notna(sp01) and pd.notna(shock_bps):\n",
    "                effect = sp01 * shock_bps\n",
    "                method = \"SpreadDelta1bp × shock_bps\"\n",
    "\n",
    "        elif market == \"fx\":\n",
    "            fx_delta = r.get(\"FXDelta\", np.nan)\n",
    "            if pd.notna(fx_delta) and pd.notna(shock_dec):\n",
    "                effect = fx_delta * shock_dec\n",
    "                method = \"FXDelta × shock_dec\"\n",
    "\n",
    "        elif market == \"other\":\n",
    "            # Cas spécifique demandé: Other + Variable = Inflation Swap\n",
    "            if isinstance(variable, str) and \"inflation swap\" in variable.lower():\n",
    "                infl01 = r.get(\"InflationDelta1bp\", np.nan)\n",
    "                if pd.notna(infl01) and pd.notna(shock_bps):\n",
    "                    effect = infl01 * shock_bps\n",
    "                    method = \"InflationDelta1bp × shock_bps (Other/Inflation Swap)\"\n",
    "\n",
    "        effects.append(effect)\n",
    "        methods.append(method)\n",
    "\n",
    "    df[\"Effect\"] = effects\n",
    "    df[\"Method\"] = methods\n",
    "    df[\"Effect\"] = df[\"Effect\"].fillna(0.0)  # pas de mapping -> effet 0\n",
    "\n",
    "    # 4) Agrégat par Portefeuille/Identifier (on conserve ISIN = first pour info)\n",
    "    agg = {\n",
    "        \"TV\": (\"TV\",\"first\"),\n",
    "        \"Duration_t\": (\"Duration\",\"first\"),\n",
    "        \"DollarRateConvexity1pc\": (\"DollarRateConvexity1pc\",\"first\"),\n",
    "        \"TotalEffect\": (\"Effect\",\"sum\"),\n",
    "        \"Counterparty\":(\"Counterparty\",\"first\")\n",
    "    }\n",
    "    if \"ISIN\" in df.columns:\n",
    "        agg[\"ISIN\"] = (\"ISIN\",\"first\")\n",
    "\n",
    "    if \"AssetClass\" in df.columns:\n",
    "        agg[\"AssetClass\"] = (\"AssetClass\",\"first\")\n",
    "\n",
    "    \n",
    "\n",
    "    per_id = df.groupby([port_col, key_col], as_index=False).agg(**agg) \n",
    "    per_id[\"TV_day\"] = per_id[\"TV\"] + per_id[\"TotalEffect\"]\n",
    "\n",
    "    # 5) (option) mise à jour de la Duration : Duration_next = Duration_t + DollarRateConvexity1pc/100 × (∑choc_rates_dec)\n",
    "    if update_duration:\n",
    "        rates_mask = df[\"Market\"].fillna(\"\").str.lower().eq(\"interest rates\")\n",
    "        rates_choc_dec = (df.loc[rates_mask]\n",
    "                            .groupby([port_col, key_col], as_index=False)[\"shock_std\"]\n",
    "                            .sum()\n",
    "                            .rename(columns={\"shock_std\":\"shock_rates_dec\"}))\n",
    "        per_id = per_id.merge(rates_choc_dec, on=[port_col, key_col], how=\"left\")\n",
    "        per_id[\"shock_rates_dec\"] = per_id[\"shock_rates_dec\"].fillna(0.0)\n",
    "        per_id[\"DollarRateConvexity1pc\"] = per_id[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "        per_id[\"Duration_t\"]      = per_id[\"Duration_t\"].fillna(0.0)\n",
    "        per_id[\"Duration_next\"]   = per_id[\"Duration_t\"] + (per_id['Duration_t']**2 -per_id[\"DollarRateConvexity1pc\"] /per_id['TV']* 10000) * per_id[\"shock_rates_dec\"]\n",
    "\n",
    "    # 6) Construire exposures_next (mise à jour TV/Duration par (Portefeuille, Identifier))\n",
    "\n",
    "\n",
    "    exposures_next = exposures.copy()\n",
    "    merge_cols = [port_col, key_col]\n",
    "    update_cols = merge_cols + [\"TV_day\"]\n",
    "    if update_duration:\n",
    "        update_cols.append(\"Duration_next\")\n",
    "        \n",
    "    exposures_next = exposures_next.merge(\n",
    "        per_id[update_cols],\n",
    "        on=merge_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    exposures_next[\"TV\"] = exposures_next[\"TV_day\"].fillna(exposures_next[\"TV\"])\n",
    "    if update_duration and \"Duration_next\" in exposures_next.columns:\n",
    "        exposures_next[\"Duration\"] = exposures_next[\"Duration_next\"].fillna(exposures_next[\"Duration\"])\n",
    "        \n",
    "    exposures_next = exposures_next.drop(columns=[c for c in [\"TV_day\", \"Duration_next\"] if c in exposures_next.columns])\n",
    "\n",
    "\n",
    "    return df, per_id, exposures_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Fichier : C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\n",
      "📦 Lignes brutes : 222\n",
      "🧹 Lignes 'TOTAL' supprimées : 2\n",
      "✅ Lignes après filtre : 220\n",
      "🔧 NaN → 0 (sensis) :\n",
      "  - RateDelta1bp          : 1\n",
      "  - SpreadDelta1bp        : 40\n",
      "  - InflationDelta1bp     : 191\n",
      "  - FXDelta               : 181\n",
      "\n",
      "⏱  Duration NaN → 0 : 10\n",
      "📐 DollarRateConvexity1pc NaN → 0 : 24\n",
      "   Portfolio  Cash_disponible\n",
      "0     900200     2.199052e+07\n",
      "1     981017     2.083247e+06\n",
      "TV avant remise à zéro par AssetClass (futures):\n",
      "            AssetClass  TV_before_reset\n",
      "0          Bond Future       3686943.19\n",
      "1  Equity Index Future             0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abenjelloun\\OneDrive - Cooperactions\\Documents\\Code\\Ponctuel\\stress_amf\\collateral_management.py:252: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[config.balance_prev_col] = merged[config.balance_prev_col].fillna(0.0)\n",
      "c:\\Users\\abenjelloun\\OneDrive - Cooperactions\\Documents\\Code\\Ponctuel\\stress_amf\\collateral_management.py:253: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[config.threshold_col] = merged[config.threshold_col].fillna(0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Charger mapping & scénarios\n",
    "pos_raw, scen_raw = load_mapping(EXCEL_MAPPING_PATH)\n",
    "scen_raw=scen_raw.iloc[:29,:]\n",
    "\n",
    "\n",
    "# 2) Charger exposures (greeks) + QC\n",
    "exposures, qc = load_exposures(EXPOSURES_PATH, return_qc=True)\n",
    "print_qc_report(qc)\n",
    "\n",
    "# Ajouter les contreparties depuis Trioptima\n",
    "cp_map = load_counterparty_mapping(TRIOPTIMA_PATH)\n",
    "exposures = exposures.merge(cp_map, on='Identifier', how='left')\n",
    "\n",
    "# Limiter POS au périmètre d'exposures avant melt\n",
    "pos_subset = exposures[['Identifier','Counterparty']].merge(pos_raw, on='Identifier', how='left')\n",
    "# 3) Préparer mapping restreint & scénarios \n",
    "pos_long = melt_pos(pos_subset)\n",
    "scen     = prepare_scenarios(scen_raw)\n",
    "merged   = merge_pos_scen(pos_long, scen)          # mapping POS ↔ scénarios\n",
    "\n",
    "\n",
    "# 4) DAY 1\n",
    "d1_det, d1_id, exp_d2 = day_step_apply(\n",
    "    exposures=exposures,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 1\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Agrégation des TV et remise à zéro pour certains futures\n",
    "updated_exp, futures_tv, balances, decisions, alerts = process_pv_after_day_1(exp_d2)\n",
    "print(\"TV avant remise à zéro par AssetClass (futures):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        print(futures_tv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio</th>\n",
       "      <th>Counterparty</th>\n",
       "      <th>Balance_J</th>\n",
       "      <th>Balance_J_1</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Seuil Déclenchement</th>\n",
       "      <th>Seuil_respecte</th>\n",
       "      <th>Appel_declenche</th>\n",
       "      <th>Sens_appel</th>\n",
       "      <th>Balance_apres_appel</th>\n",
       "      <th>Cash_initial</th>\n",
       "      <th>Cash_disponible</th>\n",
       "      <th>Cash_utilise</th>\n",
       "      <th>Cash_restant</th>\n",
       "      <th>Alerte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900200</td>\n",
       "      <td>BNPP</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (22 757 791.25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900200</td>\n",
       "      <td>CA</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.614555e+07</td>\n",
       "      <td>5.844972e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900200</td>\n",
       "      <td>CEP</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900200</td>\n",
       "      <td>GIPB</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.628448e+05</td>\n",
       "      <td>2.172767e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900200</td>\n",
       "      <td>GSOH</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.552466e+06</td>\n",
       "      <td>2.043805e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>900200</td>\n",
       "      <td>JPMSE</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>6.725968e+06</td>\n",
       "      <td>1.526455e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>900200</td>\n",
       "      <td>MSESE</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>900200</td>\n",
       "      <td>SGCIB</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.642963e+05</td>\n",
       "      <td>2.182622e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>981017</td>\n",
       "      <td>BNPP</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.061516e+06</td>\n",
       "      <td>2.173039e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>981017</td>\n",
       "      <td>CA</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>4.305859e+04</td>\n",
       "      <td>2.040188e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>981017</td>\n",
       "      <td>CEP</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (4 206 297.69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>981017</td>\n",
       "      <td>DBKAG</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>981017</td>\n",
       "      <td>GIPB</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.253084e+05</td>\n",
       "      <td>1.857938e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>981017</td>\n",
       "      <td>JPMSE</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>1.105153e+06</td>\n",
       "      <td>9.780939e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>981017</td>\n",
       "      <td>MSESE</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (24 777 464.93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>981017</td>\n",
       "      <td>NATIXIS</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (39 830 769.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>981017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Portfolio Counterparty     Balance_J  Balance_J_1     Variation  \\\n",
       "0      900200         BNPP -4.474831e+07          0.0 -4.474831e+07   \n",
       "1      900200           CA -1.614555e+07          0.0 -1.614555e+07   \n",
       "2      900200          CEP  1.106920e+05          0.0  1.106920e+05   \n",
       "3      900200         GIPB -2.628448e+05          0.0 -2.628448e+05   \n",
       "4      900200         GSOH -1.552466e+06          0.0 -1.552466e+06   \n",
       "5      900200        JPMSE -6.725968e+06          0.0 -6.725968e+06   \n",
       "6      900200        MSESE  3.627375e+05          0.0  3.627375e+05   \n",
       "7      900200        SGCIB -1.642963e+05          0.0 -1.642963e+05   \n",
       "8      900200          NaN  7.586264e+08          0.0  7.586264e+08   \n",
       "9      981017         BNPP -2.061516e+06          0.0 -2.061516e+06   \n",
       "10     981017           CA -4.305859e+04          0.0 -4.305859e+04   \n",
       "11     981017          CEP -6.289544e+06          0.0 -6.289544e+06   \n",
       "12     981017        DBKAG  5.076092e+05          0.0  5.076092e+05   \n",
       "13     981017         GIPB -2.253084e+05          0.0 -2.253084e+05   \n",
       "14     981017        JPMSE -1.105153e+06          0.0 -1.105153e+06   \n",
       "15     981017        MSESE -2.686071e+07          0.0 -2.686071e+07   \n",
       "16     981017      NATIXIS -4.191402e+07          0.0 -4.191402e+07   \n",
       "17     981017          NaN  4.175713e+08          0.0  4.175713e+08   \n",
       "\n",
       "    Seuil Déclenchement  Seuil_respecte  Appel_declenche          Sens_appel  \\\n",
       "0                   0.0           False             True      Groupama poste   \n",
       "1                   0.0           False             True      Groupama poste   \n",
       "2                   0.0           False             True  Contrepartie poste   \n",
       "3                   0.0           False             True      Groupama poste   \n",
       "4                   0.0           False             True      Groupama poste   \n",
       "5                   0.0           False             True      Groupama poste   \n",
       "6                   0.0           False             True  Contrepartie poste   \n",
       "7                   0.0           False             True      Groupama poste   \n",
       "8                   0.0           False             True  Contrepartie poste   \n",
       "9                   0.0           False             True      Groupama poste   \n",
       "10                  0.0           False             True      Groupama poste   \n",
       "11                  0.0           False             True      Groupama poste   \n",
       "12                  0.0           False             True  Contrepartie poste   \n",
       "13                  0.0           False             True      Groupama poste   \n",
       "14                  0.0           False             True      Groupama poste   \n",
       "15                  0.0           False             True      Groupama poste   \n",
       "16                  0.0           False             True      Groupama poste   \n",
       "17                  0.0           False             True  Contrepartie poste   \n",
       "\n",
       "    Balance_apres_appel  Cash_initial  Cash_disponible  Cash_utilise  \\\n",
       "0         -4.474831e+07  2.199052e+07     2.199052e+07  2.199052e+07   \n",
       "1         -1.614555e+07  2.199052e+07     2.199052e+07  1.614555e+07   \n",
       "2          1.106920e+05  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "3         -2.628448e+05  2.199052e+07     2.199052e+07  2.628448e+05   \n",
       "4         -1.552466e+06  2.199052e+07     2.199052e+07  1.552466e+06   \n",
       "5         -6.725968e+06  2.199052e+07     2.199052e+07  6.725968e+06   \n",
       "6          3.627375e+05  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "7         -1.642963e+05  2.199052e+07     2.199052e+07  1.642963e+05   \n",
       "8          7.586264e+08  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "9         -2.061516e+06  2.083247e+06     2.083247e+06  2.061516e+06   \n",
       "10        -4.305859e+04  2.083247e+06     2.083247e+06  4.305859e+04   \n",
       "11        -6.289544e+06  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "12         5.076092e+05  2.083247e+06     2.083247e+06  0.000000e+00   \n",
       "13        -2.253084e+05  2.083247e+06     2.083247e+06  2.253084e+05   \n",
       "14        -1.105153e+06  2.083247e+06     2.083247e+06  1.105153e+06   \n",
       "15        -2.686071e+07  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "16        -4.191402e+07  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "17         4.175713e+08  2.083247e+06     2.083247e+06  0.000000e+00   \n",
       "\n",
       "    Cash_restant                            Alerte  \n",
       "0   0.000000e+00  cash insuffisant (22 757 791.25)  \n",
       "1   5.844972e+06                               NaN  \n",
       "2   2.199052e+07                               NaN  \n",
       "3   2.172767e+07                               NaN  \n",
       "4   2.043805e+07                               NaN  \n",
       "5   1.526455e+07                               NaN  \n",
       "6   2.199052e+07                               NaN  \n",
       "7   2.182622e+07                               NaN  \n",
       "8   2.199052e+07                               NaN  \n",
       "9   2.173039e+04                               NaN  \n",
       "10  2.040188e+06                               NaN  \n",
       "11  0.000000e+00   cash insuffisant (4 206 297.69)  \n",
       "12  2.083247e+06                               NaN  \n",
       "13  1.857938e+06                               NaN  \n",
       "14  9.780939e+05                               NaN  \n",
       "15  0.000000e+00  cash insuffisant (24 777 464.93)  \n",
       "16  0.000000e+00  cash insuffisant (39 830 769.98)  \n",
       "17  2.083247e+06                               NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    'AssetType',\n",
    "    'Sector1',\n",
    "    'Seniority',\n",
    "    'CompositeBroadRating',\n",
    "    'MaturityDate',\n",
    "    'Maturity',\n",
    "    'Maturity Band',\n",
    "    'EffectiveMaturityDate',\n",
    "    'LiquidityScore',\n",
    "    'Country',\n",
    "    '{Class_Rating}',\n",
    "]\n",
    "def merge_day1_positions(exposures: pd.DataFrame, day1_per_id: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine current exposures with Day‑1 results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposures : pd.DataFrame\n",
    "        DataFrame containing the current positions. Must include an\n",
    "        ``Identifier`` column so that classification fields can be merged.\n",
    "    day1_per_id : pd.DataFrame\n",
    "        ``per_id`` DataFrame returned by :func:`day_step_apply`. If the\n",
    "        identifier column is named ``Identifier`` it will be normalised to\n",
    "        ``d1_id``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Day‑1 positions enriched with classification columns and a\n",
    "        ``TV_change`` column equal to ``TV - TV_day`` when both are available.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'Identifier' in day1_per_id.columns and 'd1_id' not in day1_per_id.columns:\n",
    "        day1_per_id = day1_per_id.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    class_cols = [c for c in COLUMNS_TO_KEEP if c in exposures.columns]\n",
    "    class_df = exposures[['Identifier'] + class_cols].drop_duplicates('Identifier')\n",
    "    class_df = class_df.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    merged = day1_per_id.merge(class_df, on='d1_id', how='left')\n",
    "\n",
    "    if 'TV' in merged.columns and 'TV_day' in merged.columns:\n",
    "        merged['TV_change'] = merged['TV'] - merged['TV_day']\n",
    "\n",
    "    return merged\n",
    "\n",
    "def aggregate_positions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate positions by the available classification columns.\n",
    "\n",
    "    All numeric trade value columns found in the dataframe are summed. The\n",
    "    function looks for ``TV`` (current value), ``TV_day`` (day-one value) and the\n",
    "    computed ``TV_change``. When none of these columns are present a simple\n",
    "    count per group is returned instead.\n",
    "    \"\"\"\n",
    "\n",
    "    group_cols = [col for col in COLUMNS_TO_KEEP if col in df.columns]\n",
    "\n",
    "    agg_spec = {}\n",
    "    for col in ('TV', 'TV_day', 'TV_change'):\n",
    "        if col in df.columns:\n",
    "            agg_spec[col] = 'sum'\n",
    "\n",
    "    if agg_spec:\n",
    "        agg = df.groupby(group_cols, dropna=False).agg(agg_spec).reset_index()\n",
    "    else:\n",
    "        agg = df.groupby(group_cols, dropna=False).size().reset_index(name='count')\n",
    "\n",
    "    return agg\n",
    "\n",
    "def process_positions_df(exposures: pd.DataFrame, d1_id: pd.DataFrame, aggregate: bool = False) -> pd.DataFrame:\n",
    "    #Merge Day 1 results from ``day_step_apply`` with exposures.\n",
    "    df = merge_day1_positions(exposures, d1_id)\n",
    "    if aggregate:\n",
    "        df = aggregate_positions(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_positions_df(pos_subset, d1_id).to_csv('Resultats_day1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_det.to_csv('details_Day1.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country groupings for government bond aggregation\n",
    "EUROZONE_LOW_RISK = {\n",
    "    'Austria', 'Belgium', 'Finland', 'Germany', 'Ireland', 'Latvia', 'Luxembourg', 'Netherlands', 'Slovenia'\n",
    "}\n",
    "EUROZONE_MEDIUM_RISK = {\n",
    "    'Croatia', 'Cyprus', 'France', 'Lithuania', 'Malta', 'Portugal', 'Slovakia'\n",
    "}\n",
    "EUROZONE_HIGH_RISK = {\n",
    "    'Greece', 'Italy', 'Spain'\n",
    "}\n",
    "\n",
    "Emerging_MARKETS = {\n",
    "    'Argentina', 'Brazil', 'Chile', 'China', 'Colombia', 'India', 'Indonesia', 'Mexico', 'Peru', 'South Africa', 'Turkey'\n",
    "}\n",
    "\n",
    "Advanced_economics_MARKETS = {'United States', 'Canada' ,\n",
    "                               'United Kingdom', 'Germany', 'France', 'Italy', 'Spain', 'Netherlands', 'Sweden', 'Switzerland', 'Norway', 'Austria', 'Belgium', 'Denmark', 'Finland', 'Ireland', 'Portugal', 'Greece', 'Czech Republic', 'Slovakia', 'Slovenia', 'Estonia', 'Latvia', 'Lithuania', 'Luxembourg', 'Croatia', 'Cyprus', 'Malta','Andorra', 'San Marino',\n",
    "                              'Japan', 'South Korea', 'Australia', 'New Zealand', 'Singapore', 'Hong Kong SAR', 'Taiwan', 'Macao SAR',\n",
    "                            'Israel', 'Iceland', 'Puerto Rico' }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
