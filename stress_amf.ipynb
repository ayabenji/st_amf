{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "# AMF Stress Test — workflow JOUR PAR JOUR (base = exposures)\n",
    "# ================================================================\n",
    "# Ce script :\n",
    "# 1) charge le mapping (POS + scenario_paths),\n",
    "# 2) charge 'exposures' (greeks) + nettoie/contrôle qualité (QC),\n",
    "# 3) restreint les scénarios au périmètre d'exposures (par Identifier),\n",
    "# 4) applique les chocs pour un jour donné (day_step_apply),\n",
    "# 5) renvoie un 'exposures_next' prêt à être MODIFIÉ avant le jour suivant.\n",
    "# ------------------------------------------------\n",
    "# CLÉ DE MERGE / AGRÉGATION = Identifier  (ISIN conservé pour info)\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collateral_management import process_pv_after_day_1, roll_balance_for_next_day\n",
    "\n",
    "# =======================\n",
    "# CONFIG — à adapter\n",
    "# =======================\n",
    "EXCEL_MAPPING_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\Matrices correspondance_AB.xlsx\"\n",
    "SHEET_POS = \"Test_Aya\"   # entêtes ligne 2 -> header=1\n",
    "SHEET_SCEN =  \"scenario_paths\"\n",
    "POS_HEADER_ROW = 1               # 0-based\n",
    "\n",
    "EXPOSURES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\"\n",
    "TRIOPTIMA_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\search_groupama-am_2025-03-31.xlsx\"\n",
    "COLLATERAL_BALANCES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\Collat_Cash_MTM_LU_20250401.csv\" \n",
    "COLLATERAL_BALANCE_DAY_COL = \"Balance J\"\n",
    "COLLATERAL_BALANCE_PREV_COL = \"Balance J-1\"\n",
    "COLLATERAL_THRESHOLD_COL = \"Seuil déclenchement\"\n",
    "\n",
    "# Jours possibles (doivent exister dans scenario_paths)\n",
    "DAYS = [\"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "# Règles de méthode\n",
    "INCLUDE_OTHER_INFLATION_SWAP = True\n",
    "\n",
    "# =======================\n",
    "# Utils colonnes / texte\n",
    "# =======================\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie des colonnes Excel (espaces, 'Unnamed', points).\"\"\"\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c is None or (isinstance(c, str) and c.lower().startswith(\"unnamed\")):\n",
    "            new_cols.append(None); continue\n",
    "        s = str(c).strip().replace(\"\\u00A0\", \" \")\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace(\". \", \" \").replace(\".\", \" \")\n",
    "        new_cols.append(s)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def _norm_str(x):\n",
    "    \"\"\"Normalise légèrement une chaîne (pour Market / Variable).\"\"\"\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().replace(\"\\u00A0\", \" \")\n",
    "    s = \" \".join(s.split()).replace(\". \", \" \").replace(\".\", \" \")\n",
    "    return s\n",
    "\n",
    "# =======================\n",
    "# Chargement mapping (POS + scen)\n",
    "# =======================\n",
    "POS_BASE_COLS = [\n",
    "    \"Identifier\",\"ISIN\",\"Counterparty\",\"Description\",\"Currency\",\"AssetType\",\"Sector1\",\"Seniority\",\n",
    "    \"CompositeBroadRating\",\"MaturityDate\",\"Maturity\",\"Maturity Band\",\"EffectiveMaturityDate\",\n",
    "    \"LiquidityScore\",\"Country\",\"{Class_Rating}\"\n",
    "]\n",
    "POS_MV_PAIRS = [(f\"Market {i}\", f\"Variable {i}\") for i in range(1, 7)]\n",
    "SCEN_BASE_COLS = [\"Market\",\"Variable\",\"Comment\",\"Type\",\"Unit\",\"T0\",\n",
    "                  \"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "def load_mapping(path_excel: str|Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lit la feuille POS et scenario_paths.\"\"\"\n",
    "    xls = pd.ExcelFile(path_excel)\n",
    "    pos  = pd.read_excel(xls, SHEET_POS, header=POS_HEADER_ROW)\n",
    "    scen = pd.read_excel(xls, SHEET_SCEN)\n",
    "    pos  = _clean_cols(pos)\n",
    "    scen = _clean_cols(scen)\n",
    "    return pos, scen\n",
    "\n",
    "def melt_pos(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforme POS en format long : 1 ligne par (Market k, Variable k) non vide.\"\"\"\n",
    "    pos = pos.copy()\n",
    "    for mk, vk in POS_MV_PAIRS:\n",
    "        if mk in pos.columns: pos[mk] = pos[mk].apply(_norm_str)\n",
    "        if vk in pos.columns: pos[vk] = pos[vk].apply(_norm_str)\n",
    "\n",
    "    base_cols = [c for c in POS_BASE_COLS if c in pos.columns]\n",
    "    mv_pairs_present = [(mk,vk) for mk,vk in POS_MV_PAIRS if mk in pos.columns and vk in pos.columns]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in pos.iterrows():\n",
    "        base = {c: row.get(c, np.nan) for c in base_cols}\n",
    "        for mk, vk in mv_pairs_present:\n",
    "            market, variable = row[mk], row[vk]\n",
    "            if pd.notna(market) and str(market) != \"\":\n",
    "                rows.append({**base, \"Market\": market, \"Variable\": (variable if pd.notna(variable) else np.nan)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"Identifier\" not in out.columns: out[\"Identifier\"] = np.arange(len(out))\n",
    "    return out\n",
    "\n",
    "def prepare_scenarios(scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie la table scenario_paths & conserve les colonnes utiles.\"\"\"\n",
    "    scen = scen.copy()\n",
    "    scen = scen[[c for c in SCEN_BASE_COLS if c in scen.columns]]\n",
    "    scen[\"Market\"] = scen[\"Market\"].apply(_norm_str)\n",
    "    if \"Variable\" in scen.columns: scen[\"Variable\"] = scen[\"Variable\"].apply(_norm_str)\n",
    "    if \"Type\" in scen.columns:     scen[\"Type\"]     = scen[\"Type\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    if \"Unit\" in scen.columns:     scen[\"Unit\"]     = scen[\"Unit\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    return scen\n",
    "\n",
    "def merge_pos_scen(pos_long: pd.DataFrame, scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Relie POS (long) aux scénarios : jointure (Market,Variable) ; si Variable vide -> jointure sur Market seul.\"\"\"\n",
    "    left_mv = pos_long.dropna(subset=[\"Market\",\"Variable\"]) if \"Variable\" in pos_long.columns else pos_long.copy()\n",
    "    mv_merge = left_mv.merge(scen, on=[\"Market\",\"Variable\"], how=\"left\")\n",
    "    if \"Variable\" in pos_long.columns: left_m = pos_long[pos_long[\"Variable\"].isna()].copy()\n",
    "    else:                               left_m = pd.DataFrame(columns=pos_long.columns)\n",
    "    if not left_m.empty:\n",
    "        m_merge = left_m.merge(scen.drop(columns=[\"Variable\"], errors=\"ignore\"), on=\"Market\", how=\"left\")\n",
    "        return pd.concat([mv_merge, m_merge], ignore_index=True)\n",
    "    return mv_merge\n",
    "\n",
    "def available_days(scen: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Retourne la liste des colonnes 'Day n' disponibles.\"\"\"\n",
    "    return [c for c in scen.columns if isinstance(c, str) and c.lower().startswith(\"day\")]\n",
    "\n",
    "# =======================\n",
    "# Limiter les scénarios au périmètre d'exposures (clé = Identifier)\n",
    "# =======================\n",
    "def restrict_scenarios_to_exposures(merged_mapping: pd.DataFrame,\n",
    "                                    exposures: pd.DataFrame,\n",
    "                                    key_col: str = \"Identifier\") -> pd.DataFrame:\n",
    "    \"\"\"Garde uniquement les lignes de mapping dont la clé existe dans exposures.\"\"\"\n",
    "    if key_col not in merged_mapping.columns or key_col not in exposures.columns:\n",
    "        return merged_mapping\n",
    "    keys = exposures[key_col].astype(str).unique()\n",
    "    return merged_mapping[merged_mapping[key_col].astype(str).isin(keys)].copy()\n",
    "\n",
    "# =======================\n",
    "# Standardisation des chocs\n",
    "# =======================\n",
    "def standardize_shock(value, unit: str|None) -> float|None:\n",
    "    \"\"\"Choc standardisé en décimal: 50 bps -> 0.005 ; -10% -> -0.10 ; 2 p.p -> 0.02.\"\"\"\n",
    "    if pd.isna(value): return None\n",
    "    try: val = float(value)\n",
    "    except: return None\n",
    "    if unit is None: return val\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return val / 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\"]:\n",
    "        return val / 100.0\n",
    "    return val\n",
    "\n",
    "def _to_bps(shock_std: float, unit: str|None) -> float:\n",
    "    \"\"\"Convertit un choc standardisé en bps numériques si besoin (pour PV01 / CS01 / Infl01).\"\"\"\n",
    "    if shock_std is None or pd.isna(shock_std) or unit is None: return np.nan\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return shock_std * 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\",\"pc\"]:\n",
    "        return shock_std * 10_000.0\n",
    "    return np.nan\n",
    "\n",
    "def build_daily_shocks(merged: pd.DataFrame, day_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Prépare les chocs d'un jour (inclut Identifier & ISIN si présents).\"\"\"\n",
    "    if day_col not in merged.columns:\n",
    "        raise ValueError(f\"Jour '{day_col}' introuvable. Jours dispo: {available_days(merged)}\")\n",
    "    out = merged.copy()\n",
    "    out[\"shock_raw\"] = out[day_col]\n",
    "    out[\"shock_std\"] = [standardize_shock(v, u) for v, u in zip(out[\"shock_raw\"], out.get(\"Unit\", pd.Series([None]*len(out))))]\n",
    "\n",
    "    keep = [\"Identifier\",\"ISIN\",\"Market\",\"Variable\",\"Type\",\"Unit\",\"T0\", day_col, \"shock_std\",\"Comment\"]\n",
    "    keep = [c for c in keep if c in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# =======================\n",
    "# Chargement exposures + QC\n",
    "# =======================\n",
    "def load_exposures(path_excel: str|Path, return_qc: bool = True):\n",
    "    \"\"\"Charge le fichier d'expositions, nettoie et met des valeurs par défaut pour éviter les NaN.\"\"\"\n",
    "    df = pd.read_csv(path_excel,sep=';',decimal='.')\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    qc = {\"path\": str(path_excel)}\n",
    "    qc[\"rows_raw\"] = int(df.shape[0])\n",
    "\n",
    "    # Convertir en numérique les colonnes clés si elles existent\n",
    "    num_cols = [\n",
    "        \"TV\",\"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\n",
    "        \"RateDelta1bp\",\"RateVega\",\"SpreadDelta1bp\",\"CreditVega\",\n",
    "        \"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\",\n",
    "        \"Nominal\",\"TVPercent\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Enlever les lignes agrégées 'TOTAL' si jamais elles existent\n",
    "    removed_total = 0\n",
    "    if \"AssetID\" in df.columns:\n",
    "        mask_total = df[\"AssetID\"].astype(str).str.upper().eq(\"TOTAL\")\n",
    "        removed_total = int(mask_total.sum())\n",
    "        df = df[~mask_total].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    qc[\"rows_removed_total\"] = removed_total\n",
    "    qc[\"rows_after_filter\"] = int(df.shape[0])\n",
    "\n",
    "    mask_0 = df['Nominal'] == 0\n",
    "    removed_n_0 = int(mask_0.sum())\n",
    "    df = df[~mask_0].copy()\n",
    "\n",
    "    qc['row_removes_notional_null'] = removed_n_0\n",
    "\n",
    "    # Sensi NaN -> 0 (évite les effets NaN)\n",
    "    sensi_zero = [\n",
    "        \"RateDelta1bp\",\"SpreadDelta1bp\",\"InflationDelta1bp\",\n",
    "        \"EquityDelta\",\"FXDelta\",\"RateVega\",\"CreditVega\",\"EquityVega\",\"FXVega\",\"EquityGamma\"\n",
    "    ]\n",
    "    qc[\"filled_zero\"] = {}\n",
    "    for c in sensi_zero:\n",
    "        if c in df.columns:\n",
    "            n = int(df[c].isna().sum())\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "            qc[\"filled_zero\"][c] = n\n",
    "\n",
    "    # Duration NaN -> 0\n",
    "    if \"Duration\" in df.columns:\n",
    "        qc[\"duration_filled_zero\"] = int(df[\"Duration\"].isna().sum())\n",
    "        df[\"Duration\"] = df[\"Duration\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"duration_missing_col\"] = True\n",
    "        df[\"Duration\"] = 0.0\n",
    "\n",
    "    # DollarRateConvexity1pc : NaN -> 0\n",
    "    if \"DollarRateConvexity1pc\" in df.columns:\n",
    "        qc[\"dollarconvexity_filled_zero\"] = int(df[\"DollarRateConvexity1pc\"].isna().sum())\n",
    "        df[\"DollarRateConvexity1pc\"] = df[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"dollarconvexity_missing_col\"] = True\n",
    "        df[\"DollarRateConvexity1pc\"] = 0.0\n",
    "\n",
    "    return (df, qc) if return_qc else df\n",
    "\n",
    "def print_qc_report(qc: dict):\n",
    "    \"\"\"Affiche un petit rapport de nettoyage des expositions.\"\"\"\n",
    "    print(f\"📄 Fichier : {qc.get('path','')}\")\n",
    "    print(f\"📦 Lignes brutes : {qc['rows_raw']}\")\n",
    "    print(f\"🧹 Lignes 'TOTAL' supprimées : {qc['rows_removed_total']}\")\n",
    "    print(f\"✅ Lignes après filtre : {qc['rows_after_filter']}\")\n",
    "    print(\"🔧 NaN → 0 (sensis) :\")\n",
    "    for k, v in qc[\"filled_zero\"].items():\n",
    "        print(f\"  - {k:<22}: {v}\")\n",
    "    if \"duration_filled_zero\" in qc:\n",
    "        print(f\"\\n⏱  Duration NaN → 0 : {qc['duration_filled_zero']}\")\n",
    "    if qc.get(\"duration_missing_col\"):\n",
    "        print(\"⚠️  Colonne 'Duration' manquante → créée à 0\")\n",
    "    if qc.get(\"dollarconvexity_missing_col\"):\n",
    "        print(\"⚠️  Colonne 'DollarRateConvexity1pc' manquante → créée\")\n",
    "    if \"dollarconvexity_filled_zero\" in qc:\n",
    "        print(f\"📐 DollarRateConvexity1pc NaN → 0 : {qc['dollarconvexity_filled_zero']}\")\n",
    "        \n",
    "def load_counterparty_mapping(path_excel: str|Path, id_col='FREE_TEXT_1', cp_col='CP_ORIG') -> pd.DataFrame:\n",
    "    \"\"\"Lit le fichier Trioptima et renvoie un mapping Identifier→Counterparty.\"\"\"\n",
    "    df = pd.read_excel(path_excel)\n",
    "    df = _clean_cols(df)\n",
    "    if id_col not in df.columns or cp_col not in df.columns:\n",
    "        raise KeyError(f\"Colonnes '{id_col}' ou '{cp_col}' manquantes dans {path_excel}\")\n",
    "    return (df[[id_col, cp_col]]\n",
    "            .dropna(subset=[id_col])\n",
    "            .rename(columns={id_col: 'Identifier', cp_col: 'Counterparty'})\n",
    "            .drop_duplicates('Identifier'))\n",
    "\n",
    "# =======================\n",
    "# Étape \"un jour\" — base = exposures, clé = Identifier\n",
    "# =======================\n",
    "def day_step_apply(\n",
    "    exposures: pd.DataFrame,\n",
    "    merged_mapping: pd.DataFrame,\n",
    "    day_col: str = \"Day 1\",\n",
    "    include_other_inflation_swap: bool = True,\n",
    "    update_duration: bool = True,   # Duration_next = Duration_t + DollarRateConvexity1pc/100 × shock_rates_dec\n",
    "    update_tv: bool = True,         # TV_day = TV + TotalEffect\n",
    "    return_pivot: bool = True,\n",
    "    key_col: str = \"Identifier\",    # clé de jointure\n",
    "    port_col: str = \"Portfolio\",    # colonne de portefeuille\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique les chocs d'un jour et renvoie:\n",
    "      - detailed       : lignes (Portfolio × Identifier × Market × Variable) avec Effect/Method (+ ISIN si dispo)\n",
    "      - per_id         : agrégat par Portefeuille/Identifier (ISIN=first), TV_day, (Duration_next si update_duration)\n",
    "      - exposures_next : copie de exposures avec TV/Duration mises à jour (selon flags)\n",
    "      - pivot          : (optionnel) large des Effects ('Market :: Variable'), index = (Portefeuille, Identifier)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Chocs du jour (standardisés en décimal)\n",
    "    shocks_day = build_daily_shocks(merged_mapping, day_col=day_col)  # contient Identifier, ISIN (POS), Market, Variable, shock_std...\n",
    "    shocks_day = shocks_day.drop_duplicates(subset=[key_col, \"Market\", \"Variable\"])\n",
    "    # 2) Merge en LEFT depuis exposures (la base de calcul) par Identifier et ajoute le Portefeuille\n",
    "    cols_needed = [\n",
    "        key_col, port_col, \"AssetClass\",\"Country\",\"AssetID\",\"Nominal\",\"TVPercent\",\"TV\",\n",
    "        \"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\"RateDelta1bp\",\"RateVega\",\n",
    "        \"SpreadDelta1bp\",\"CreditVega\",\"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\", \"Counterparty\"\n",
    "    ]\n",
    "    cols_needed = [c for c in cols_needed if c in exposures.columns]\n",
    "    base = exposures[cols_needed].copy()\n",
    "    if key_col not in base.columns or key_col not in shocks_day.columns:\n",
    "        raise KeyError(f\"Clé '{key_col}' absente de exposures ou du mapping de scénarios.\")\n",
    "\n",
    "    df = base.merge(shocks_day, on=key_col, how=\"left\")  # garde les instruments sans mapping (effet=0)\n",
    "\n",
    "    # 3) Calcul des effets par ligne (Portfolio × Identifier × Market × Variable)\n",
    "    effects, methods = [], []\n",
    "    for _, r in df.iterrows():\n",
    "        market_raw = r.get(\"Market\")\n",
    "        if isinstance(market_raw, str):\n",
    "            market = market_raw.lower()\n",
    "        elif pd.notna(market_raw):\n",
    "            market = str(market_raw).lower()\n",
    "        else:\n",
    "            market = \"\"\n",
    "        variable = (r.get(\"Variable\") or \"\")\n",
    "        unit     = r.get(\"Unit\")\n",
    "        shock_std = r.get(\"shock_std\", np.nan)\n",
    "\n",
    "        shock_bps = _to_bps(shock_std, unit)  # pour PV01/CS01/Infl01 (en bps numériques)\n",
    "        shock_dec = shock_std                 # décimal (ex: +50 bps -> +0.005)\n",
    "\n",
    "        effect = np.nan\n",
    "        method = None\n",
    "\n",
    "        if market == \"equity\":\n",
    "            eq_delta = r.get(\"EquityDelta\", np.nan)\n",
    "            if pd.notna(eq_delta) and pd.notna(shock_dec):\n",
    "                effect = eq_delta * shock_dec\n",
    "                method = \"EquityDelta × shock_dec\"\n",
    "\n",
    "        elif market == \"interest rates\":\n",
    "            if pd.notna(r.get(\"RateDelta1bp\")) and pd.notna(shock_bps):\n",
    "                effect = r[\"RateDelta1bp\"] * shock_bps\n",
    "                method = \"RateDelta1bp × shock_bps\"\n",
    "\n",
    "        elif \"spread\" in market:  # couvre Gov Spreads / Corp Spreads (peu importe la casse)\n",
    "            sp01 = r.get(\"SpreadDelta1bp\", np.nan)\n",
    "            if pd.notna(sp01) and pd.notna(shock_bps):\n",
    "                effect = sp01 * shock_bps\n",
    "                method = \"SpreadDelta1bp × shock_bps\"\n",
    "\n",
    "        elif market == \"fx\":\n",
    "            fx_delta = r.get(\"FXDelta\", np.nan)\n",
    "            if pd.notna(fx_delta) and pd.notna(shock_dec):\n",
    "                effect = fx_delta * shock_dec\n",
    "                method = \"FXDelta × shock_dec\"\n",
    "\n",
    "        elif market == \"other\":\n",
    "            # Cas spécifique demandé: Other + Variable = Inflation Swap\n",
    "            if isinstance(variable, str) and \"inflation swap\" in variable.lower():\n",
    "                infl01 = r.get(\"InflationDelta1bp\", np.nan)\n",
    "                if pd.notna(infl01) and pd.notna(shock_bps):\n",
    "                    effect = infl01 * shock_bps\n",
    "                    method = \"InflationDelta1bp × shock_bps (Other/Inflation Swap)\"\n",
    "\n",
    "        effects.append(effect)\n",
    "        methods.append(method)\n",
    "\n",
    "    df[\"Effect\"] = effects\n",
    "    df[\"Method\"] = methods\n",
    "    df[\"Effect\"] = df[\"Effect\"].fillna(0.0)  # pas de mapping -> effet 0\n",
    "\n",
    "    # 4) Agrégat par Portefeuille/Identifier (on conserve ISIN = first pour info)\n",
    "    agg = {\n",
    "        \"TV\": (\"TV\",\"first\"),\n",
    "        \"Duration_t\": (\"Duration\",\"first\"),\n",
    "        \"DollarRateConvexity1pc\": (\"DollarRateConvexity1pc\",\"first\"),\n",
    "        \"TotalEffect\": (\"Effect\",\"sum\"),\n",
    "        \"Counterparty\":(\"Counterparty\",\"first\")\n",
    "    }\n",
    "    if \"ISIN\" in df.columns:\n",
    "        agg[\"ISIN\"] = (\"ISIN\",\"first\")\n",
    "\n",
    "    if \"AssetClass\" in df.columns:\n",
    "        agg[\"AssetClass\"] = (\"AssetClass\",\"first\")\n",
    "\n",
    "    \n",
    "\n",
    "    per_id = df.groupby([port_col, key_col], as_index=False).agg(**agg) \n",
    "    per_id[\"TV_day\"] = per_id[\"TV\"] + per_id[\"TotalEffect\"]\n",
    "\n",
    "    # 5) (option) mise à jour de la Duration : Duration_next = Duration_t + DollarRateConvexity1pc/100 × (∑choc_rates_dec)\n",
    "    if update_duration:\n",
    "        rates_mask = df[\"Market\"].fillna(\"\").str.lower().eq(\"interest rates\")\n",
    "        rates_choc_dec = (df.loc[rates_mask]\n",
    "                            .groupby([port_col, key_col], as_index=False)[\"shock_std\"]\n",
    "                            .sum()\n",
    "                            .rename(columns={\"shock_std\":\"shock_rates_dec\"}))\n",
    "        per_id = per_id.merge(rates_choc_dec, on=[port_col, key_col], how=\"left\")\n",
    "        per_id[\"shock_rates_dec\"] = per_id[\"shock_rates_dec\"].fillna(0.0)\n",
    "        per_id[\"DollarRateConvexity1pc\"] = per_id[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "        per_id[\"Duration_t\"]      = per_id[\"Duration_t\"].fillna(0.0)\n",
    "        per_id[\"Duration_next\"]   = per_id[\"Duration_t\"] + (per_id['Duration_t']**2 -per_id[\"DollarRateConvexity1pc\"] /per_id['TV']* 10000) * per_id[\"shock_rates_dec\"]\n",
    "\n",
    "    # 6) Construire exposures_next (mise à jour TV/Duration par (Portefeuille, Identifier))\n",
    "\n",
    "\n",
    "    exposures_next = exposures.copy()\n",
    "    merge_cols = [port_col, key_col]\n",
    "    update_cols = merge_cols + [\"TV_day\"]\n",
    "    if update_duration:\n",
    "        update_cols.append(\"Duration_next\")\n",
    "        \n",
    "    exposures_next = exposures_next.merge(\n",
    "        per_id[update_cols],\n",
    "        on=merge_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    exposures_next[\"TV\"] = exposures_next[\"TV_day\"].fillna(exposures_next[\"TV\"])\n",
    "    if update_duration and \"Duration_next\" in exposures_next.columns:\n",
    "        exposures_next[\"Duration\"] = exposures_next[\"Duration_next\"].fillna(exposures_next[\"Duration\"])\n",
    "        \n",
    "    exposures_next = exposures_next.drop(columns=[c for c in [\"TV_day\", \"Duration_next\"] if c in exposures_next.columns])\n",
    "\n",
    "\n",
    "    return df, per_id, exposures_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Fichier : C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\Périmètre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\n",
      "📦 Lignes brutes : 222\n",
      "🧹 Lignes 'TOTAL' supprimées : 2\n",
      "✅ Lignes après filtre : 220\n",
      "🔧 NaN → 0 (sensis) :\n",
      "  - RateDelta1bp          : 1\n",
      "  - SpreadDelta1bp        : 40\n",
      "  - InflationDelta1bp     : 191\n",
      "  - FXDelta               : 181\n",
      "\n",
      "⏱  Duration NaN → 0 : 10\n",
      "📐 DollarRateConvexity1pc NaN → 0 : 24\n",
      "   Portfolio  Cash_disponible\n",
      "0     900200     2.199052e+07\n",
      "1     981017     2.083247e+06\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m\n\u001b[0;32m     23\u001b[0m d1_det, d1_id, exp_d2 \u001b[38;5;241m=\u001b[39m day_step_apply(\n\u001b[0;32m     24\u001b[0m     exposures\u001b[38;5;241m=\u001b[39mexposures,\n\u001b[0;32m     25\u001b[0m     merged_mapping\u001b[38;5;241m=\u001b[39mmerged,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     port_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPortfolio\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Agrégation des TV et remise à zéro pour certains futures\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m updated_exp, futures_tv, balances, decisions, alerts \u001b[38;5;241m=\u001b[39m process_pv_after_day_1(exp_d2)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV avant remise à zéro par AssetClass (futures):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39moption_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.float_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat):\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\OneDrive - Cooperactions\\Documents\\Code\\Ponctuel\\stress_amf\\collateral_management.py:259\u001b[0m, in \u001b[0;36mprocess_pv_after_day_1\u001b[1;34m(exposures_next, future_classes, cash_identifier, config, history_date)\u001b[0m\n\u001b[0;32m    255\u001b[0m merged[config\u001b[38;5;241m.\u001b[39mcash_col] \u001b[38;5;241m=\u001b[39m merged[config\u001b[38;5;241m.\u001b[39mcash_col]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    257\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39msort_values([config\u001b[38;5;241m.\u001b[39mportfolio_col, config\u001b[38;5;241m.\u001b[39mcounterparty_col])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 259\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalance_J\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m merged[config\u001b[38;5;241m.\u001b[39mbalance_prev_col]\n\u001b[0;32m    260\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeuil_respecte\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m merged[config\u001b[38;5;241m.\u001b[39mthreshold_col]\n\u001b[0;32m    261\u001b[0m merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppel_declenche\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mmerged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeuil_respecte\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abenjelloun\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Charger mapping & scénarios\n",
    "pos_raw, scen_raw = load_mapping(EXCEL_MAPPING_PATH)\n",
    "scen_raw=scen_raw.iloc[:29,:]\n",
    "\n",
    "\n",
    "# 2) Charger exposures (greeks) + QC\n",
    "exposures, qc = load_exposures(EXPOSURES_PATH, return_qc=True)\n",
    "print_qc_report(qc)\n",
    "\n",
    "# Ajouter les contreparties depuis Trioptima\n",
    "cp_map = load_counterparty_mapping(TRIOPTIMA_PATH)\n",
    "exposures = exposures.merge(cp_map, on='Identifier', how='left')\n",
    "\n",
    "# Limiter POS au périmètre d'exposures avant melt\n",
    "pos_subset = exposures[['Identifier','Counterparty']].merge(pos_raw, on='Identifier', how='left')\n",
    "# 3) Préparer mapping restreint & scénarios \n",
    "pos_long = melt_pos(pos_subset)\n",
    "scen     = prepare_scenarios(scen_raw)\n",
    "merged   = merge_pos_scen(pos_long, scen)          # mapping POS ↔ scénarios\n",
    "\n",
    "\n",
    "# 4) DAY 1\n",
    "d1_det, d1_id, exp_d2 = day_step_apply(\n",
    "    exposures=exposures,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 1\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Agrégation des TV et remise à zéro pour certains futures\n",
    "updated_exp, futures_tv, balances, decisions, alerts = process_pv_after_day_1(exp_d2)\n",
    "print(\"TV avant remise à zéro par AssetClass (futures):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        print(futures_tv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions.to_csv(\"collateral_decisions.csv\", index=False,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_id.to_csv(\"d1_id.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_d2.to_csv(\"exp_d2.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    'AssetType',\n",
    "    'Sector1',\n",
    "    'Seniority',\n",
    "    'CompositeBroadRating',\n",
    "    'MaturityDate',\n",
    "    'Maturity',\n",
    "    'Maturity Band',\n",
    "    'EffectiveMaturityDate',\n",
    "    'LiquidityScore',\n",
    "    'Country',\n",
    "    '{Class_Rating}',\n",
    "]\n",
    "def merge_day1_positions(exposures: pd.DataFrame, day1_per_id: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine current exposures with Day‑1 results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposures : pd.DataFrame\n",
    "        DataFrame containing the current positions. Must include an\n",
    "        ``Identifier`` column so that classification fields can be merged.\n",
    "    day1_per_id : pd.DataFrame\n",
    "        ``per_id`` DataFrame returned by :func:`day_step_apply`. If the\n",
    "        identifier column is named ``Identifier`` it will be normalised to\n",
    "        ``d1_id``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Day‑1 positions enriched with classification columns and a\n",
    "        ``TV_change`` column equal to ``TV - TV_day`` when both are available.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'Identifier' in day1_per_id.columns and 'd1_id' not in day1_per_id.columns:\n",
    "        day1_per_id = day1_per_id.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    class_cols = [c for c in COLUMNS_TO_KEEP if c in exposures.columns]\n",
    "    class_df = exposures[['Identifier'] + class_cols].drop_duplicates('Identifier')\n",
    "    class_df = class_df.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    merged = day1_per_id.merge(class_df, on='d1_id', how='left')\n",
    "\n",
    "    if 'TV' in merged.columns and 'TV_day' in merged.columns:\n",
    "        merged['TV_change'] = merged['TV'] - merged['TV_day']\n",
    "\n",
    "    return merged\n",
    "\n",
    "def aggregate_positions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate positions by the available classification columns.\n",
    "\n",
    "    All numeric trade value columns found in the dataframe are summed. The\n",
    "    function looks for ``TV`` (current value), ``TV_day`` (day-one value) and the\n",
    "    computed ``TV_change``. When none of these columns are present a simple\n",
    "    count per group is returned instead.\n",
    "    \"\"\"\n",
    "\n",
    "    group_cols = [col for col in COLUMNS_TO_KEEP if col in df.columns]\n",
    "\n",
    "    agg_spec = {}\n",
    "    for col in ('TV', 'TV_day', 'TV_change'):\n",
    "        if col in df.columns:\n",
    "            agg_spec[col] = 'sum'\n",
    "\n",
    "    if agg_spec:\n",
    "        agg = df.groupby(group_cols, dropna=False).agg(agg_spec).reset_index()\n",
    "    else:\n",
    "        agg = df.groupby(group_cols, dropna=False).size().reset_index(name='count')\n",
    "\n",
    "    return agg\n",
    "\n",
    "def process_positions_df(exposures: pd.DataFrame, d1_id: pd.DataFrame, aggregate: bool = False) -> pd.DataFrame:\n",
    "    #Merge Day 1 results from ``day_step_apply`` with exposures.\n",
    "    df = merge_day1_positions(exposures, d1_id)\n",
    "    if aggregate:\n",
    "        df = aggregate_positions(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country groupings for government bond aggregation\n",
    "EUROZONE_LOW_RISK = {\n",
    "    'Austria', 'Belgium', 'Finland', 'Germany', 'Ireland', 'Latvia', 'Luxembourg', 'Netherlands', 'Slovenia'\n",
    "}\n",
    "EUROZONE_MEDIUM_RISK = {\n",
    "    'Croatia', 'Cyprus', 'France', 'Lithuania', 'Malta', 'Portugal', 'Slovakia'\n",
    "}\n",
    "EUROZONE_HIGH_RISK = {\n",
    "    'Greece', 'Italy', 'Spain'\n",
    "}\n",
    "\n",
    "Emerging_MARKETS = {\n",
    "    'Argentina', 'Brazil', 'Chile', 'China', 'Colombia', 'India', 'Indonesia', 'Mexico', 'Peru', 'South Africa', 'Turkey'\n",
    "}\n",
    "\n",
    "Advanced_economics_MARKETS = {'United States', 'Canada' ,\n",
    "                               'United Kingdom', 'Germany', 'France', 'Italy', 'Spain', 'Netherlands', 'Sweden', 'Switzerland', 'Norway', 'Austria', 'Belgium', 'Denmark', 'Finland', 'Ireland', 'Portugal', 'Greece', 'Czech Republic', 'Slovakia', 'Slovenia', 'Estonia', 'Latvia', 'Lithuania', 'Luxembourg', 'Croatia', 'Cyprus', 'Malta','Andorra', 'San Marino',\n",
    "                              'Japan', 'South Korea', 'Australia', 'New Zealand', 'Singapore', 'Hong Kong SAR', 'Taiwan', 'Macao SAR',\n",
    "                            'Israel', 'Iceland', 'Puerto Rico' }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
