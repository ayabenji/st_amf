{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ================================================================\n",
    "# AMF Stress Test ‚Äî workflow JOUR PAR JOUR (base = exposures)\n",
    "# ================================================================\n",
    "# Ce script :\n",
    "# 1) charge le mapping (POS + scenario_paths),\n",
    "# 2) charge 'exposures' (greeks) + nettoie/contr√¥le qualit√© (QC),\n",
    "# 3) restreint les sc√©narios au p√©rim√®tre d'exposures (par Identifier),\n",
    "# 4) applique les chocs pour un jour donn√© (day_step_apply),\n",
    "# 5) renvoie un 'exposures_next' pr√™t √† √™tre MODIFI√â avant le jour suivant.\n",
    "# ------------------------------------------------\n",
    "# CL√â DE MERGE / AGR√âGATION = Identifier  (ISIN conserv√© pour info)\n",
    "# ------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collateral_management import process_pv_after_day_1, roll_balance_for_next_day\n",
    "\n",
    "# =======================\n",
    "# CONFIG ‚Äî √† adapter\n",
    "# =======================\n",
    "EXCEL_MAPPING_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\Matrices correspondance_AB.xlsx\"\n",
    "SHEET_POS = \"Test_Aya\"   # ent√™tes ligne 2 -> header=1\n",
    "SHEET_SCEN =  \"scenario_paths\"\n",
    "POS_HEADER_ROW = 1               # 0-based\n",
    "\n",
    "EXPOSURES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\"\n",
    "TRIOPTIMA_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\search_groupama-am_2025-03-31.xlsx\"\n",
    "COLLATERAL_BALANCES_PATH = r\"C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\Collat_Cash_MTM_LU_20250401.csv\" \n",
    "COLLATERAL_BALANCE_DAY_COL = \"Balance J\"\n",
    "COLLATERAL_BALANCE_PREV_COL = \"Balance J-1\"\n",
    "COLLATERAL_THRESHOLD_COL = \"Seuil d√©clenchement\"\n",
    "\n",
    "# Jours possibles (doivent exister dans scenario_paths)\n",
    "DAYS = [\"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "# R√®gles de m√©thode\n",
    "INCLUDE_OTHER_INFLATION_SWAP = True\n",
    "\n",
    "# =======================\n",
    "# Utils colonnes / texte\n",
    "# =======================\n",
    "def _clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie des colonnes Excel (espaces, 'Unnamed', points).\"\"\"\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c is None or (isinstance(c, str) and c.lower().startswith(\"unnamed\")):\n",
    "            new_cols.append(None); continue\n",
    "        s = str(c).strip().replace(\"\\u00A0\", \" \")\n",
    "        s = \" \".join(s.split())\n",
    "        s = s.replace(\". \", \" \").replace(\".\", \" \")\n",
    "        new_cols.append(s)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def _norm_str(x):\n",
    "    \"\"\"Normalise l√©g√®rement une cha√Æne (pour Market / Variable).\"\"\"\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().replace(\"\\u00A0\", \" \")\n",
    "    s = \" \".join(s.split()).replace(\". \", \" \").replace(\".\", \" \")\n",
    "    return s\n",
    "\n",
    "# =======================\n",
    "# Chargement mapping (POS + scen)\n",
    "# =======================\n",
    "POS_BASE_COLS = [\n",
    "    \"Identifier\",\"ISIN\",\"Counterparty\",\"Description\",\"Currency\",\"AssetType\",\"Sector1\",\"Seniority\",\n",
    "    \"CompositeBroadRating\",\"MaturityDate\",\"Maturity\",\"Maturity Band\",\"EffectiveMaturityDate\",\n",
    "    \"LiquidityScore\",\"Country\",\"{Class_Rating}\"\n",
    "]\n",
    "POS_MV_PAIRS = [(f\"Market {i}\", f\"Variable {i}\") for i in range(1, 7)]\n",
    "SCEN_BASE_COLS = [\"Market\",\"Variable\",\"Comment\",\"Type\",\"Unit\",\"T0\",\n",
    "                  \"Day 1\",\"Day 2\",\"Day 3\",\"Day 4\",\"Day 5\",\"Day 10\"]\n",
    "\n",
    "def load_mapping(path_excel: str|Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Lit la feuille POS et scenario_paths.\"\"\"\n",
    "    xls = pd.ExcelFile(path_excel)\n",
    "    pos  = pd.read_excel(xls, SHEET_POS, header=POS_HEADER_ROW)\n",
    "    scen = pd.read_excel(xls, SHEET_SCEN)\n",
    "    pos  = _clean_cols(pos)\n",
    "    scen = _clean_cols(scen)\n",
    "    return pos, scen\n",
    "\n",
    "def melt_pos(pos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforme POS en format long : 1 ligne par (Market k, Variable k) non vide.\"\"\"\n",
    "    pos = pos.copy()\n",
    "    for mk, vk in POS_MV_PAIRS:\n",
    "        if mk in pos.columns: pos[mk] = pos[mk].apply(_norm_str)\n",
    "        if vk in pos.columns: pos[vk] = pos[vk].apply(_norm_str)\n",
    "\n",
    "    base_cols = [c for c in POS_BASE_COLS if c in pos.columns]\n",
    "    mv_pairs_present = [(mk,vk) for mk,vk in POS_MV_PAIRS if mk in pos.columns and vk in pos.columns]\n",
    "\n",
    "    rows = []\n",
    "    for _, row in pos.iterrows():\n",
    "        base = {c: row.get(c, np.nan) for c in base_cols}\n",
    "        for mk, vk in mv_pairs_present:\n",
    "            market, variable = row[mk], row[vk]\n",
    "            if pd.notna(market) and str(market) != \"\":\n",
    "                rows.append({**base, \"Market\": market, \"Variable\": (variable if pd.notna(variable) else np.nan)})\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"Identifier\" not in out.columns: out[\"Identifier\"] = np.arange(len(out))\n",
    "    return out\n",
    "\n",
    "def prepare_scenarios(scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie la table scenario_paths & conserve les colonnes utiles.\"\"\"\n",
    "    scen = scen.copy()\n",
    "    scen = scen[[c for c in SCEN_BASE_COLS if c in scen.columns]]\n",
    "    scen[\"Market\"] = scen[\"Market\"].apply(_norm_str)\n",
    "    if \"Variable\" in scen.columns: scen[\"Variable\"] = scen[\"Variable\"].apply(_norm_str)\n",
    "    if \"Type\" in scen.columns:     scen[\"Type\"]     = scen[\"Type\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    if \"Unit\" in scen.columns:     scen[\"Unit\"]     = scen[\"Unit\"].apply(lambda x: str(x).strip().lower() if pd.notna(x) else x)\n",
    "    return scen\n",
    "\n",
    "def merge_pos_scen(pos_long: pd.DataFrame, scen: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Relie POS (long) aux sc√©narios : jointure (Market,Variable) ; si Variable vide -> jointure sur Market seul.\"\"\"\n",
    "    left_mv = pos_long.dropna(subset=[\"Market\",\"Variable\"]) if \"Variable\" in pos_long.columns else pos_long.copy()\n",
    "    mv_merge = left_mv.merge(scen, on=[\"Market\",\"Variable\"], how=\"left\")\n",
    "    if \"Variable\" in pos_long.columns: left_m = pos_long[pos_long[\"Variable\"].isna()].copy()\n",
    "    else:                               left_m = pd.DataFrame(columns=pos_long.columns)\n",
    "    if not left_m.empty:\n",
    "        m_merge = left_m.merge(scen.drop(columns=[\"Variable\"], errors=\"ignore\"), on=\"Market\", how=\"left\")\n",
    "        return pd.concat([mv_merge, m_merge], ignore_index=True)\n",
    "    return mv_merge\n",
    "\n",
    "def available_days(scen: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Retourne la liste des colonnes 'Day n' disponibles.\"\"\"\n",
    "    return [c for c in scen.columns if isinstance(c, str) and c.lower().startswith(\"day\")]\n",
    "\n",
    "# =======================\n",
    "# Limiter les sc√©narios au p√©rim√®tre d'exposures (cl√© = Identifier)\n",
    "# =======================\n",
    "def restrict_scenarios_to_exposures(merged_mapping: pd.DataFrame,\n",
    "                                    exposures: pd.DataFrame,\n",
    "                                    key_col: str = \"Identifier\") -> pd.DataFrame:\n",
    "    \"\"\"Garde uniquement les lignes de mapping dont la cl√© existe dans exposures.\"\"\"\n",
    "    if key_col not in merged_mapping.columns or key_col not in exposures.columns:\n",
    "        return merged_mapping\n",
    "    keys = exposures[key_col].astype(str).unique()\n",
    "    return merged_mapping[merged_mapping[key_col].astype(str).isin(keys)].copy()\n",
    "\n",
    "# =======================\n",
    "# Standardisation des chocs\n",
    "# =======================\n",
    "def standardize_shock(value, unit: str|None) -> float|None:\n",
    "    \"\"\"Choc standardis√© en d√©cimal: 50 bps -> 0.005 ; -10% -> -0.10 ; 2 p.p -> 0.02.\"\"\"\n",
    "    if pd.isna(value): return None\n",
    "    try: val = float(value)\n",
    "    except: return None\n",
    "    if unit is None: return val\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return val / 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\"]:\n",
    "        return val / 100.0\n",
    "    return val\n",
    "\n",
    "def _to_bps(shock_std: float, unit: str|None) -> float:\n",
    "    \"\"\"Convertit un choc standardis√© en bps num√©riques si besoin (pour PV01 / CS01 / Infl01).\"\"\"\n",
    "    if shock_std is None or pd.isna(shock_std) or unit is None: return np.nan\n",
    "    u = unit.lower()\n",
    "    if u in [\"bp\",\"bps\"]: return shock_std * 10_000.0\n",
    "    if u in [\"%\",\"percent\",\"percentage\",\"p.p\",\"pp\",\"ppt\",\"percentage point\",\"percentage points\",\"pc\"]:\n",
    "        return shock_std * 10_000.0\n",
    "    return np.nan\n",
    "\n",
    "def build_daily_shocks(merged: pd.DataFrame, day_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Pr√©pare les chocs d'un jour (inclut Identifier & ISIN si pr√©sents).\"\"\"\n",
    "    if day_col not in merged.columns:\n",
    "        raise ValueError(f\"Jour '{day_col}' introuvable. Jours dispo: {available_days(merged)}\")\n",
    "    out = merged.copy()\n",
    "    out[\"shock_raw\"] = out[day_col]\n",
    "    out[\"shock_std\"] = [standardize_shock(v, u) for v, u in zip(out[\"shock_raw\"], out.get(\"Unit\", pd.Series([None]*len(out))))]\n",
    "\n",
    "    keep = [\"Identifier\",\"ISIN\",\"Market\",\"Variable\",\"Type\",\"Unit\",\"T0\", day_col, \"shock_std\",\"Comment\"]\n",
    "    keep = [c for c in keep if c in out.columns]\n",
    "    return out[keep]\n",
    "\n",
    "# =======================\n",
    "# Chargement exposures + QC\n",
    "# =======================\n",
    "def load_exposures(path_excel: str|Path, return_qc: bool = True):\n",
    "    \"\"\"Charge le fichier d'expositions, nettoie et met des valeurs par d√©faut pour √©viter les NaN.\"\"\"\n",
    "    df = pd.read_csv(path_excel,sep=';',decimal='.')\n",
    "    df = _clean_cols(df)\n",
    "\n",
    "    qc = {\"path\": str(path_excel)}\n",
    "    qc[\"rows_raw\"] = int(df.shape[0])\n",
    "\n",
    "    # Convertir en num√©rique les colonnes cl√©s si elles existent\n",
    "    num_cols = [\n",
    "        \"TV\",\"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\n",
    "        \"RateDelta1bp\",\"RateVega\",\"SpreadDelta1bp\",\"CreditVega\",\n",
    "        \"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\",\n",
    "        \"Nominal\",\"TVPercent\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Enlever les lignes agr√©g√©es 'TOTAL' si jamais elles existent\n",
    "    removed_total = 0\n",
    "    if \"AssetID\" in df.columns:\n",
    "        mask_total = df[\"AssetID\"].astype(str).str.upper().eq(\"TOTAL\")\n",
    "        removed_total = int(mask_total.sum())\n",
    "        df = df[~mask_total].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    qc[\"rows_removed_total\"] = removed_total\n",
    "    qc[\"rows_after_filter\"] = int(df.shape[0])\n",
    "\n",
    "    mask_0 = df['Nominal'] == 0\n",
    "    removed_n_0 = int(mask_0.sum())\n",
    "    df = df[~mask_0].copy()\n",
    "\n",
    "    qc['row_removes_notional_null'] = removed_n_0\n",
    "\n",
    "    # Sensi NaN -> 0 (√©vite les effets NaN)\n",
    "    sensi_zero = [\n",
    "        \"RateDelta1bp\",\"SpreadDelta1bp\",\"InflationDelta1bp\",\n",
    "        \"EquityDelta\",\"FXDelta\",\"RateVega\",\"CreditVega\",\"EquityVega\",\"FXVega\",\"EquityGamma\"\n",
    "    ]\n",
    "    qc[\"filled_zero\"] = {}\n",
    "    for c in sensi_zero:\n",
    "        if c in df.columns:\n",
    "            n = int(df[c].isna().sum())\n",
    "            df[c] = df[c].fillna(0.0)\n",
    "            qc[\"filled_zero\"][c] = n\n",
    "\n",
    "    # Duration NaN -> 0\n",
    "    if \"Duration\" in df.columns:\n",
    "        qc[\"duration_filled_zero\"] = int(df[\"Duration\"].isna().sum())\n",
    "        df[\"Duration\"] = df[\"Duration\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"duration_missing_col\"] = True\n",
    "        df[\"Duration\"] = 0.0\n",
    "\n",
    "    # DollarRateConvexity1pc : NaN -> 0\n",
    "    if \"DollarRateConvexity1pc\" in df.columns:\n",
    "        qc[\"dollarconvexity_filled_zero\"] = int(df[\"DollarRateConvexity1pc\"].isna().sum())\n",
    "        df[\"DollarRateConvexity1pc\"] = df[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "    else:\n",
    "        qc[\"dollarconvexity_missing_col\"] = True\n",
    "        df[\"DollarRateConvexity1pc\"] = 0.0\n",
    "\n",
    "    return (df, qc) if return_qc else df\n",
    "\n",
    "def print_qc_report(qc: dict):\n",
    "    \"\"\"Affiche un petit rapport de nettoyage des expositions.\"\"\"\n",
    "    print(f\"üìÑ Fichier : {qc.get('path','')}\")\n",
    "    print(f\"üì¶ Lignes brutes : {qc['rows_raw']}\")\n",
    "    print(f\"üßπ Lignes 'TOTAL' supprim√©es : {qc['rows_removed_total']}\")\n",
    "    print(f\"‚úÖ Lignes apr√®s filtre : {qc['rows_after_filter']}\")\n",
    "    print(\"üîß NaN ‚Üí 0 (sensis) :\")\n",
    "    for k, v in qc[\"filled_zero\"].items():\n",
    "        print(f\"  - {k:<22}: {v}\")\n",
    "    if \"duration_filled_zero\" in qc:\n",
    "        print(f\"\\n‚è±  Duration NaN ‚Üí 0 : {qc['duration_filled_zero']}\")\n",
    "    if qc.get(\"duration_missing_col\"):\n",
    "        print(\"‚ö†Ô∏è  Colonne 'Duration' manquante ‚Üí cr√©√©e √† 0\")\n",
    "    if qc.get(\"dollarconvexity_missing_col\"):\n",
    "        print(\"‚ö†Ô∏è  Colonne 'DollarRateConvexity1pc' manquante ‚Üí cr√©√©e\")\n",
    "    if \"dollarconvexity_filled_zero\" in qc:\n",
    "        print(f\"üìê DollarRateConvexity1pc NaN ‚Üí 0 : {qc['dollarconvexity_filled_zero']}\")\n",
    "        \n",
    "def load_counterparty_mapping(path_excel: str|Path, id_col='FREE_TEXT_1', cp_col='CP') -> pd.DataFrame:\n",
    "    \"\"\"Lit le fichier Trioptima et renvoie un mapping Identifier‚ÜíCounterparty.\"\"\"\n",
    "    df = pd.read_excel(path_excel)\n",
    "    df = _clean_cols(df)\n",
    "    if id_col not in df.columns or cp_col not in df.columns:\n",
    "        raise KeyError(f\"Colonnes '{id_col}' ou '{cp_col}' manquantes dans {path_excel}\")\n",
    "    return (df[[id_col, cp_col]]\n",
    "            .dropna(subset=[id_col])\n",
    "            .rename(columns={id_col: 'Identifier', cp_col: 'Counterparty'})\n",
    "            .drop_duplicates('Identifier'))\n",
    "\n",
    "# =======================\n",
    "# √âtape \"un jour\" ‚Äî base = exposures, cl√© = Identifier\n",
    "# =======================\n",
    "def day_step_apply(\n",
    "    exposures: pd.DataFrame,\n",
    "    merged_mapping: pd.DataFrame,\n",
    "    day_col: str = \"Day 1\",\n",
    "    include_other_inflation_swap: bool = True,\n",
    "    update_duration: bool = True,   # Duration_next = Duration_t + DollarRateConvexity1pc/100 √ó shock_rates_dec\n",
    "    update_tv: bool = True,         # TV_day = TV + TotalEffect\n",
    "    return_pivot: bool = True,\n",
    "    key_col: str = \"Identifier\",    # cl√© de jointure\n",
    "    port_col: str = \"Portfolio\",    # colonne de portefeuille\n",
    "):\n",
    "    \"\"\"\n",
    "    Applique les chocs d'un jour et renvoie:\n",
    "      - detailed       : lignes (Portfolio √ó Identifier √ó Market √ó Variable) avec Effect/Method (+ ISIN si dispo)\n",
    "      - per_id         : agr√©gat par Portefeuille/Identifier (ISIN=first), TV_day, (Duration_next si update_duration)\n",
    "      - exposures_next : copie de exposures avec TV/Duration mises √† jour (selon flags)\n",
    "      - pivot          : (optionnel) large des Effects ('Market :: Variable'), index = (Portefeuille, Identifier)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Chocs du jour (standardis√©s en d√©cimal)\n",
    "    shocks_day = build_daily_shocks(merged_mapping, day_col=day_col)  # contient Identifier, ISIN (POS), Market, Variable, shock_std...\n",
    "    shocks_day = shocks_day.drop_duplicates(subset=[key_col, \"Market\", \"Variable\"])\n",
    "    # 2) Merge en LEFT depuis exposures (la base de calcul) par Identifier et ajoute le Portefeuille\n",
    "    cols_needed = [\n",
    "        key_col, port_col, \"AssetClass\",\"Country\",\"AssetID\",\"Nominal\",\"TVPercent\",\"TV\",\n",
    "        \"MacaulayDuration\",\"Duration\",\"DollarRateConvexity1pc\",\"RateDelta1bp\",\"RateVega\",\n",
    "        \"SpreadDelta1bp\",\"CreditVega\",\"EquityDelta\",\"EquityGamma\",\"EquityVega\",\"FXDelta\",\"FXVega\",\"InflationDelta1bp\", \"Counterparty\"\n",
    "    ]\n",
    "    cols_needed = [c for c in cols_needed if c in exposures.columns]\n",
    "    base = exposures[cols_needed].copy()\n",
    "    if key_col not in base.columns or key_col not in shocks_day.columns:\n",
    "        raise KeyError(f\"Cl√© '{key_col}' absente de exposures ou du mapping de sc√©narios.\")\n",
    "\n",
    "    df = base.merge(shocks_day, on=key_col, how=\"left\")  # garde les instruments sans mapping (effet=0)\n",
    "\n",
    "    # 3) Calcul des effets par ligne (Portfolio √ó Identifier √ó Market √ó Variable)\n",
    "    effects, methods = [], []\n",
    "    for _, r in df.iterrows():\n",
    "        market_raw = r.get(\"Market\")\n",
    "        if isinstance(market_raw, str):\n",
    "            market = market_raw.lower()\n",
    "        elif pd.notna(market_raw):\n",
    "            market = str(market_raw).lower()\n",
    "        else:\n",
    "            market = \"\"\n",
    "        variable = (r.get(\"Variable\") or \"\")\n",
    "        unit     = r.get(\"Unit\")\n",
    "        shock_std = r.get(\"shock_std\", np.nan)\n",
    "\n",
    "        shock_bps = _to_bps(shock_std, unit)  # pour PV01/CS01/Infl01 (en bps num√©riques)\n",
    "        shock_dec = shock_std                 # d√©cimal (ex: +50 bps -> +0.005)\n",
    "\n",
    "        effect = np.nan\n",
    "        method = None\n",
    "\n",
    "        if market == \"equity\":\n",
    "            eq_delta = r.get(\"EquityDelta\", np.nan)\n",
    "            if pd.notna(eq_delta) and pd.notna(shock_dec):\n",
    "                effect = eq_delta * shock_dec\n",
    "                method = \"EquityDelta √ó shock_dec\"\n",
    "\n",
    "        elif market == \"interest rates\":\n",
    "            if pd.notna(r.get(\"RateDelta1bp\")) and pd.notna(shock_bps):\n",
    "                effect = r[\"RateDelta1bp\"] * shock_bps\n",
    "                method = \"RateDelta1bp √ó shock_bps\"\n",
    "\n",
    "        elif \"spread\" in market:  # couvre Gov Spreads / Corp Spreads (peu importe la casse)\n",
    "            sp01 = r.get(\"SpreadDelta1bp\", np.nan)\n",
    "            if pd.notna(sp01) and pd.notna(shock_bps):\n",
    "                effect = sp01 * shock_bps\n",
    "                method = \"SpreadDelta1bp √ó shock_bps\"\n",
    "\n",
    "        elif market == \"fx\":\n",
    "            fx_delta = r.get(\"FXDelta\", np.nan)\n",
    "            if pd.notna(fx_delta) and pd.notna(shock_dec):\n",
    "                effect = fx_delta * shock_dec\n",
    "                method = \"FXDelta √ó shock_dec\"\n",
    "\n",
    "        elif market == \"other\":\n",
    "            # Cas sp√©cifique demand√©: Other + Variable = Inflation Swap\n",
    "            if isinstance(variable, str) and \"inflation swap\" in variable.lower():\n",
    "                infl01 = r.get(\"InflationDelta1bp\", np.nan)\n",
    "                if pd.notna(infl01) and pd.notna(shock_bps):\n",
    "                    effect = infl01 * shock_bps\n",
    "                    method = \"InflationDelta1bp √ó shock_bps (Other/Inflation Swap)\"\n",
    "\n",
    "        effects.append(effect)\n",
    "        methods.append(method)\n",
    "\n",
    "    df[\"Effect\"] = effects\n",
    "    df[\"Method\"] = methods\n",
    "    df[\"Effect\"] = df[\"Effect\"].fillna(0.0)  # pas de mapping -> effet 0\n",
    "\n",
    "    # 4) Agr√©gat par Portefeuille/Identifier (on conserve ISIN = first pour info)\n",
    "    agg = {\n",
    "        \"TV\": (\"TV\",\"first\"),\n",
    "        \"Duration_t\": (\"Duration\",\"first\"),\n",
    "        \"DollarRateConvexity1pc\": (\"DollarRateConvexity1pc\",\"first\"),\n",
    "        \"TotalEffect\": (\"Effect\",\"sum\"),\n",
    "        \"Counterparty\":(\"Counterparty\",\"first\")\n",
    "    }\n",
    "    if \"ISIN\" in df.columns:\n",
    "        agg[\"ISIN\"] = (\"ISIN\",\"first\")\n",
    "\n",
    "    if \"AssetClass\" in df.columns:\n",
    "        agg[\"AssetClass\"] = (\"AssetClass\",\"first\")\n",
    "\n",
    "    \n",
    "\n",
    "    per_id = df.groupby([port_col, key_col], as_index=False).agg(**agg) \n",
    "    per_id[\"TV_day\"] = per_id[\"TV\"] + per_id[\"TotalEffect\"]\n",
    "\n",
    "    # 5) (option) mise √† jour de la Duration : Duration_next = Duration_t + DollarRateConvexity1pc/100 √ó (‚àëchoc_rates_dec)\n",
    "    if update_duration:\n",
    "        rates_mask = df[\"Market\"].fillna(\"\").str.lower().eq(\"interest rates\")\n",
    "        rates_choc_dec = (df.loc[rates_mask]\n",
    "                            .groupby([port_col, key_col], as_index=False)[\"shock_std\"]\n",
    "                            .sum()\n",
    "                            .rename(columns={\"shock_std\":\"shock_rates_dec\"}))\n",
    "        per_id = per_id.merge(rates_choc_dec, on=[port_col, key_col], how=\"left\")\n",
    "        per_id[\"shock_rates_dec\"] = per_id[\"shock_rates_dec\"].fillna(0.0)\n",
    "        per_id[\"DollarRateConvexity1pc\"] = per_id[\"DollarRateConvexity1pc\"].fillna(0.0)\n",
    "        per_id[\"Duration_t\"]      = per_id[\"Duration_t\"].fillna(0.0)\n",
    "        per_id[\"Duration_next\"]   = per_id[\"Duration_t\"] + (per_id['Duration_t']**2 -per_id[\"DollarRateConvexity1pc\"] /per_id['TV']* 10000) * per_id[\"shock_rates_dec\"]\n",
    "\n",
    "    # 6) Construire exposures_next (mise √† jour TV/Duration par (Portefeuille, Identifier))\n",
    "\n",
    "\n",
    "    exposures_next = exposures.copy()\n",
    "    merge_cols = [port_col, key_col]\n",
    "    update_cols = merge_cols + [\"TV_day\"]\n",
    "    if update_duration:\n",
    "        update_cols.append(\"Duration_next\")\n",
    "        \n",
    "    exposures_next = exposures_next.merge(\n",
    "        per_id[update_cols],\n",
    "        on=merge_cols,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    exposures_next[\"TV\"] = exposures_next[\"TV_day\"].fillna(exposures_next[\"TV\"])\n",
    "    if update_duration and \"Duration_next\" in exposures_next.columns:\n",
    "        exposures_next[\"Duration\"] = exposures_next[\"Duration_next\"].fillna(exposures_next[\"Duration\"])\n",
    "        \n",
    "    exposures_next = exposures_next.drop(columns=[c for c in [\"TV_day\", \"Duration_next\"] if c in exposures_next.columns])\n",
    "\n",
    "\n",
    "    return df, per_id, exposures_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Fichier : C:\\Users\\abenjelloun\\OneDrive - Cooperactions\\GAM-E-Risk Perf - RMP\\1.PROD\\1.REGLEMENTAIRE\\14.Stress Test AMF (JB)\\Production\\P√©rim√®tre et positions\\GROUPAMA-BreakoutsOverTime-2025-03-31.csv\n",
      "üì¶ Lignes brutes : 222\n",
      "üßπ Lignes 'TOTAL' supprim√©es : 2\n",
      "‚úÖ Lignes apr√®s filtre : 220\n",
      "üîß NaN ‚Üí 0 (sensis) :\n",
      "  - RateDelta1bp          : 1\n",
      "  - SpreadDelta1bp        : 40\n",
      "  - InflationDelta1bp     : 191\n",
      "  - FXDelta               : 181\n",
      "\n",
      "‚è±  Duration NaN ‚Üí 0 : 10\n",
      "üìê DollarRateConvexity1pc NaN ‚Üí 0 : 24\n",
      "   Portfolio  Cash_disponible\n",
      "0     900200     2.199052e+07\n",
      "1     981017     2.083247e+06\n",
      "TV avant remise √† z√©ro par AssetClass (futures):\n",
      "            AssetClass  TV_before_reset\n",
      "0          Bond Future       3686943.19\n",
      "1  Equity Index Future             0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abenjelloun\\OneDrive - Cooperactions\\Documents\\Code\\Ponctuel\\stress_amf\\collateral_management.py:252: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[config.balance_prev_col] = merged[config.balance_prev_col].fillna(0.0)\n",
      "c:\\Users\\abenjelloun\\OneDrive - Cooperactions\\Documents\\Code\\Ponctuel\\stress_amf\\collateral_management.py:253: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[config.threshold_col] = merged[config.threshold_col].fillna(0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Charger mapping & sc√©narios\n",
    "pos_raw, scen_raw = load_mapping(EXCEL_MAPPING_PATH)\n",
    "scen_raw=scen_raw.iloc[:29,:]\n",
    "\n",
    "\n",
    "# 2) Charger exposures (greeks) + QC\n",
    "exposures, qc = load_exposures(EXPOSURES_PATH, return_qc=True)\n",
    "print_qc_report(qc)\n",
    "\n",
    "# Ajouter les contreparties depuis Trioptima\n",
    "cp_map = load_counterparty_mapping(TRIOPTIMA_PATH)\n",
    "exposures = exposures.merge(cp_map, on='Identifier', how='left')\n",
    "\n",
    "# Limiter POS au p√©rim√®tre d'exposures avant melt\n",
    "pos_subset = exposures[['Identifier','Counterparty']].merge(pos_raw, on='Identifier', how='left')\n",
    "# 3) Pr√©parer mapping restreint & sc√©narios \n",
    "pos_long = melt_pos(pos_subset)\n",
    "scen     = prepare_scenarios(scen_raw)\n",
    "merged   = merge_pos_scen(pos_long, scen)          # mapping POS ‚Üî sc√©narios\n",
    "\n",
    "\n",
    "# 4) DAY 1\n",
    "d1_det, d1_id, exp_d2 = day_step_apply(\n",
    "    exposures=exposures,\n",
    "    merged_mapping=merged,\n",
    "    day_col=\"Day 1\",\n",
    "    include_other_inflation_swap=INCLUDE_OTHER_INFLATION_SWAP,\n",
    "    update_duration=True,\n",
    "    update_tv=True,\n",
    "    return_pivot=True,\n",
    "    key_col=\"Identifier\",\n",
    "    port_col=\"Portfolio\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Agr√©gation des TV et remise √† z√©ro pour certains futures\n",
    "updated_exp, futures_tv, balances, decisions, alerts = process_pv_after_day_1(exp_d2)\n",
    "print(\"TV avant remise √† z√©ro par AssetClass (futures):\")\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        print(futures_tv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio</th>\n",
       "      <th>Counterparty</th>\n",
       "      <th>Balance_J</th>\n",
       "      <th>Balance_J_1</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Seuil D√©clenchement</th>\n",
       "      <th>Seuil_respecte</th>\n",
       "      <th>Appel_declenche</th>\n",
       "      <th>Sens_appel</th>\n",
       "      <th>Balance_apres_appel</th>\n",
       "      <th>Cash_initial</th>\n",
       "      <th>Cash_disponible</th>\n",
       "      <th>Cash_utilise</th>\n",
       "      <th>Cash_restant</th>\n",
       "      <th>Alerte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900200</td>\n",
       "      <td>BNPP</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.474831e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (22 757 791.25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900200</td>\n",
       "      <td>CA</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.614555e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.614555e+07</td>\n",
       "      <td>5.844972e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900200</td>\n",
       "      <td>CEP</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>1.106920e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900200</td>\n",
       "      <td>GIPB</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.628448e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.628448e+05</td>\n",
       "      <td>2.172767e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900200</td>\n",
       "      <td>GSOH</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.552466e+06</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.552466e+06</td>\n",
       "      <td>2.043805e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>900200</td>\n",
       "      <td>JPMSE</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-6.725968e+06</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>6.725968e+06</td>\n",
       "      <td>1.526455e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>900200</td>\n",
       "      <td>MSESE</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>3.627375e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>900200</td>\n",
       "      <td>SGCIB</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.642963e+05</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>1.642963e+05</td>\n",
       "      <td>2.182622e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>7.586264e+08</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.199052e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>981017</td>\n",
       "      <td>BNPP</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.061516e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.061516e+06</td>\n",
       "      <td>2.173039e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>981017</td>\n",
       "      <td>CA</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.305859e+04</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>4.305859e+04</td>\n",
       "      <td>2.040188e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>981017</td>\n",
       "      <td>CEP</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-6.289544e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (4 206 297.69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>981017</td>\n",
       "      <td>DBKAG</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>5.076092e+05</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>981017</td>\n",
       "      <td>GIPB</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.253084e+05</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.253084e+05</td>\n",
       "      <td>1.857938e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>981017</td>\n",
       "      <td>JPMSE</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-1.105153e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>1.105153e+06</td>\n",
       "      <td>9.780939e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>981017</td>\n",
       "      <td>MSESE</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-2.686071e+07</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (24 777 464.93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>981017</td>\n",
       "      <td>NATIXIS</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Groupama poste</td>\n",
       "      <td>-4.191402e+07</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>cash insuffisant (39 830 769.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>981017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contrepartie poste</td>\n",
       "      <td>4.175713e+08</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.083247e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Portfolio Counterparty     Balance_J  Balance_J_1     Variation  \\\n",
       "0      900200         BNPP -4.474831e+07          0.0 -4.474831e+07   \n",
       "1      900200           CA -1.614555e+07          0.0 -1.614555e+07   \n",
       "2      900200          CEP  1.106920e+05          0.0  1.106920e+05   \n",
       "3      900200         GIPB -2.628448e+05          0.0 -2.628448e+05   \n",
       "4      900200         GSOH -1.552466e+06          0.0 -1.552466e+06   \n",
       "5      900200        JPMSE -6.725968e+06          0.0 -6.725968e+06   \n",
       "6      900200        MSESE  3.627375e+05          0.0  3.627375e+05   \n",
       "7      900200        SGCIB -1.642963e+05          0.0 -1.642963e+05   \n",
       "8      900200          NaN  7.586264e+08          0.0  7.586264e+08   \n",
       "9      981017         BNPP -2.061516e+06          0.0 -2.061516e+06   \n",
       "10     981017           CA -4.305859e+04          0.0 -4.305859e+04   \n",
       "11     981017          CEP -6.289544e+06          0.0 -6.289544e+06   \n",
       "12     981017        DBKAG  5.076092e+05          0.0  5.076092e+05   \n",
       "13     981017         GIPB -2.253084e+05          0.0 -2.253084e+05   \n",
       "14     981017        JPMSE -1.105153e+06          0.0 -1.105153e+06   \n",
       "15     981017        MSESE -2.686071e+07          0.0 -2.686071e+07   \n",
       "16     981017      NATIXIS -4.191402e+07          0.0 -4.191402e+07   \n",
       "17     981017          NaN  4.175713e+08          0.0  4.175713e+08   \n",
       "\n",
       "    Seuil D√©clenchement  Seuil_respecte  Appel_declenche          Sens_appel  \\\n",
       "0                   0.0           False             True      Groupama poste   \n",
       "1                   0.0           False             True      Groupama poste   \n",
       "2                   0.0           False             True  Contrepartie poste   \n",
       "3                   0.0           False             True      Groupama poste   \n",
       "4                   0.0           False             True      Groupama poste   \n",
       "5                   0.0           False             True      Groupama poste   \n",
       "6                   0.0           False             True  Contrepartie poste   \n",
       "7                   0.0           False             True      Groupama poste   \n",
       "8                   0.0           False             True  Contrepartie poste   \n",
       "9                   0.0           False             True      Groupama poste   \n",
       "10                  0.0           False             True      Groupama poste   \n",
       "11                  0.0           False             True      Groupama poste   \n",
       "12                  0.0           False             True  Contrepartie poste   \n",
       "13                  0.0           False             True      Groupama poste   \n",
       "14                  0.0           False             True      Groupama poste   \n",
       "15                  0.0           False             True      Groupama poste   \n",
       "16                  0.0           False             True      Groupama poste   \n",
       "17                  0.0           False             True  Contrepartie poste   \n",
       "\n",
       "    Balance_apres_appel  Cash_initial  Cash_disponible  Cash_utilise  \\\n",
       "0         -4.474831e+07  2.199052e+07     2.199052e+07  2.199052e+07   \n",
       "1         -1.614555e+07  2.199052e+07     2.199052e+07  1.614555e+07   \n",
       "2          1.106920e+05  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "3         -2.628448e+05  2.199052e+07     2.199052e+07  2.628448e+05   \n",
       "4         -1.552466e+06  2.199052e+07     2.199052e+07  1.552466e+06   \n",
       "5         -6.725968e+06  2.199052e+07     2.199052e+07  6.725968e+06   \n",
       "6          3.627375e+05  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "7         -1.642963e+05  2.199052e+07     2.199052e+07  1.642963e+05   \n",
       "8          7.586264e+08  2.199052e+07     2.199052e+07  0.000000e+00   \n",
       "9         -2.061516e+06  2.083247e+06     2.083247e+06  2.061516e+06   \n",
       "10        -4.305859e+04  2.083247e+06     2.083247e+06  4.305859e+04   \n",
       "11        -6.289544e+06  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "12         5.076092e+05  2.083247e+06     2.083247e+06  0.000000e+00   \n",
       "13        -2.253084e+05  2.083247e+06     2.083247e+06  2.253084e+05   \n",
       "14        -1.105153e+06  2.083247e+06     2.083247e+06  1.105153e+06   \n",
       "15        -2.686071e+07  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "16        -4.191402e+07  2.083247e+06     2.083247e+06  2.083247e+06   \n",
       "17         4.175713e+08  2.083247e+06     2.083247e+06  0.000000e+00   \n",
       "\n",
       "    Cash_restant                            Alerte  \n",
       "0   0.000000e+00  cash insuffisant (22 757 791.25)  \n",
       "1   5.844972e+06                               NaN  \n",
       "2   2.199052e+07                               NaN  \n",
       "3   2.172767e+07                               NaN  \n",
       "4   2.043805e+07                               NaN  \n",
       "5   1.526455e+07                               NaN  \n",
       "6   2.199052e+07                               NaN  \n",
       "7   2.182622e+07                               NaN  \n",
       "8   2.199052e+07                               NaN  \n",
       "9   2.173039e+04                               NaN  \n",
       "10  2.040188e+06                               NaN  \n",
       "11  0.000000e+00   cash insuffisant (4 206 297.69)  \n",
       "12  2.083247e+06                               NaN  \n",
       "13  1.857938e+06                               NaN  \n",
       "14  9.780939e+05                               NaN  \n",
       "15  0.000000e+00  cash insuffisant (24 777 464.93)  \n",
       "16  0.000000e+00  cash insuffisant (39 830 769.98)  \n",
       "17  2.083247e+06                               NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    'AssetType',\n",
    "    'Sector1',\n",
    "    'Seniority',\n",
    "    'CompositeBroadRating',\n",
    "    'MaturityDate',\n",
    "    'Maturity',\n",
    "    'Maturity Band',\n",
    "    'EffectiveMaturityDate',\n",
    "    'LiquidityScore',\n",
    "    'Country',\n",
    "    '{Class_Rating}',\n",
    "]\n",
    "def merge_day1_positions(exposures: pd.DataFrame, day1_per_id: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine current exposures with Day‚Äë1 results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exposures : pd.DataFrame\n",
    "        DataFrame containing the current positions. Must include an\n",
    "        ``Identifier`` column so that classification fields can be merged.\n",
    "    day1_per_id : pd.DataFrame\n",
    "        ``per_id`` DataFrame returned by :func:`day_step_apply`. If the\n",
    "        identifier column is named ``Identifier`` it will be normalised to\n",
    "        ``d1_id``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Day‚Äë1 positions enriched with classification columns and a\n",
    "        ``TV_change`` column equal to ``TV - TV_day`` when both are available.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'Identifier' in day1_per_id.columns and 'd1_id' not in day1_per_id.columns:\n",
    "        day1_per_id = day1_per_id.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    class_cols = [c for c in COLUMNS_TO_KEEP if c in exposures.columns]\n",
    "    class_df = exposures[['Identifier'] + class_cols].drop_duplicates('Identifier')\n",
    "    class_df = class_df.rename(columns={'Identifier': 'd1_id'})\n",
    "\n",
    "    merged = day1_per_id.merge(class_df, on='d1_id', how='left')\n",
    "\n",
    "    if 'TV' in merged.columns and 'TV_day' in merged.columns:\n",
    "        merged['TV_change'] = merged['TV'] - merged['TV_day']\n",
    "\n",
    "    return merged\n",
    "\n",
    "def aggregate_positions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate positions by the available classification columns.\n",
    "\n",
    "    All numeric trade value columns found in the dataframe are summed. The\n",
    "    function looks for ``TV`` (current value), ``TV_day`` (day-one value) and the\n",
    "    computed ``TV_change``. When none of these columns are present a simple\n",
    "    count per group is returned instead.\n",
    "    \"\"\"\n",
    "\n",
    "    group_cols = [col for col in COLUMNS_TO_KEEP if col in df.columns]\n",
    "\n",
    "    agg_spec = {}\n",
    "    for col in ('TV', 'TV_day', 'TV_change'):\n",
    "        if col in df.columns:\n",
    "            agg_spec[col] = 'sum'\n",
    "\n",
    "    if agg_spec:\n",
    "        agg = df.groupby(group_cols, dropna=False).agg(agg_spec).reset_index()\n",
    "    else:\n",
    "        agg = df.groupby(group_cols, dropna=False).size().reset_index(name='count')\n",
    "\n",
    "    return agg\n",
    "\n",
    "def process_positions_df(exposures: pd.DataFrame, d1_id: pd.DataFrame, aggregate: bool = False) -> pd.DataFrame:\n",
    "    #Merge Day 1 results from ``day_step_apply`` with exposures.\n",
    "    df = merge_day1_positions(exposures, d1_id)\n",
    "    if aggregate:\n",
    "        df = aggregate_positions(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_positions_df(pos_subset, d1_id).to_csv('Resultats_day1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_det.to_csv('details_Day1.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country groupings for government bond aggregation\n",
    "EUROZONE_LOW_RISK = {\n",
    "    'Austria', 'Belgium', 'Finland', 'Germany', 'Ireland', 'Latvia', 'Luxembourg', 'Netherlands', 'Slovenia'\n",
    "}\n",
    "EUROZONE_MEDIUM_RISK = {\n",
    "    'Croatia', 'Cyprus', 'France', 'Lithuania', 'Malta', 'Portugal', 'Slovakia'\n",
    "}\n",
    "EUROZONE_HIGH_RISK = {\n",
    "    'Greece', 'Italy', 'Spain'\n",
    "}\n",
    "\n",
    "Emerging_MARKETS = {\n",
    "    'Argentina', 'Brazil', 'Chile', 'China', 'Colombia', 'India', 'Indonesia', 'Mexico', 'Peru', 'South Africa', 'Turkey'\n",
    "}\n",
    "\n",
    "Advanced_economics_MARKETS = {'United States', 'Canada' ,\n",
    "                               'United Kingdom', 'Germany', 'France', 'Italy', 'Spain', 'Netherlands', 'Sweden', 'Switzerland', 'Norway', 'Austria', 'Belgium', 'Denmark', 'Finland', 'Ireland', 'Portugal', 'Greece', 'Czech Republic', 'Slovakia', 'Slovenia', 'Estonia', 'Latvia', 'Lithuania', 'Luxembourg', 'Croatia', 'Cyprus', 'Malta','Andorra', 'San Marino',\n",
    "                              'Japan', 'South Korea', 'Australia', 'New Zealand', 'Singapore', 'Hong Kong SAR', 'Taiwan', 'Macao SAR',\n",
    "                            'Israel', 'Iceland', 'Puerto Rico' }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
